\documentclass[hyperref,german,beleg,final,twoside]{cgvpub}
%weitere Optionen zum Ergänzen (in eckigen Klammern):
% 
% female	weibliche Titelbezeichnung bei Diplom
% bibnum	numerische Literaturschlüssel
% final 	für Abgabe	
% lof			Abbildungsverzeichis
% lot			Tabellenverzeichnis
% noproblem	keine Aufgabenstellung
% notoc			kein Inhaltsverzeichnis
% twoside		zweiseitig

\usepackage{tabularx}

\author{Josef Schulz}
\title{Ground-Truth-Renderer für Partikelbasierte Daten}
\birthday{20. Oktober 1989}
\placeofbirth{Dresden}
\matno{3658867}

\betreuer{Dipl-MedienInf. Joachim Staib}
\bibfiles{literatur.bib}
\problem{
Die Darstellung von Partikeldaten mittels Kugelglyphen ist in der wissenschaftlichen Visualisierung
inzwischen etabliert. Gerade bei dichten Datensätzen stellen kompakte Anordnungen von sehr vielen
Kugeln jedoch ein Problem für die Erkennbarkeit der zu visualisierenden Vorgänge dar. Eine Möglichkeit, diesem Problem zu begegnen ist es, über Blinn-Phong-Beleuchtung hinausgehende Effekte wie
globale Schatten oder den Einsatz von Methoden aus dem Volume-Rendering zu integrieren. Durch
deren Komplexität muss in Echtzeitvisualisierungen jedoch auf teilweise grobe Approximationen zu-
rückgegriffen werden. Die Einschätzung der Approximationsqualität fällt häufig schwer, da keine Visualisierung des exakten Verfahrens verfügbar ist.
Ziel dieser Belegarbeit ist die Umsetzung eines CPU-Renderers für Partikeldaten, der eine Reihe
von erweiterten Visualisierungseffekten unterstützt. Er soll die Grundlage für Ground-Truth-
Visualisierungen bieten.
Zunächst soll eine geeignete Softwarearchitektur konzipiert und umgesetzt werden. Die Partikel sollen als mit lichtemittierendem und ?absorbierendem Gas gefüllte Kugeln interpretiert werden. Es sollen anschließend Methoden entwickelt werden, um einen physikalisch plausiblen globalen Schattenwurf und Lichttransport für eine beliebige Anzahl an Punkt- und Richtungslichtquellen zu ermöglichen.
Die dafür notwendigen Gleichungen für Kugeln mit konstanter Dichte und Emission, sowie linearer
Absorption, sollen soweit wie möglich analytisch bestimmt und, sobald nicht mehr möglich, mittels
möglichst exakter numerischer Integratoren ausgewertet werden.

Die Teilaufgaben umfassen:

\begin{itemize}
\item Umfassende Literaturrecherche zur globalen Beleuchtungsrechnung in der Volumen Visualisierung
\item Schrittweise Konzeption und Umsetzung einer erweiterbaren Architektur zum Erzeugen von Ground-Truth-Bildern:

	\begin{enumerate}
		\item Zunächst als Raytracer für opake Kugeln, der globale Schatteneffekte von frei
		positionierbaren Punkt- und Richtungslichtquellen unterstützt
		\item Umsetzung eines Renderers, der Kugeln als Volumen nach dem Emissions-Absorptions-Modell rendert, dabei analytische Bestimmung des Volume-Rendering-Integrals, einschließlich Integration direkter Beleuchtung unverdeckter Lichtquellen
		\item Erweiterung zu verdeckten Lichtquellen und Bestimmung der Lichtstärke- und Farbe
		für Lichtstrahlen durch verdeckende Kugeln
	\end{enumerate}
\item Unterstützung für ein Standardformat wie VRML
\item Evaluation in Bezug auf Korrektheit, Bildartefakte und (numerische) Grenzfälle
\end{itemize}
\newpage
Optional:
\begin{itemize}
\item Unterstützung für Refraktionseffekte
\item Unterstützung komplexerer Materialtypen
\end{itemize}
}
\copyrighterklaerung{Hier soll jeder Autor die von ihm eingeholten
Zustimmungen der Copyright-Besitzer angeben bzw. die in Web Press
Rooms angegebenen generellen Konditionen seiner Text- und
Bild"ubernahmen zitieren.}
\acknowledgments{Die Danksagung...}

\begin{document}

\chapter{Einleitung}
 
	Ein Problem der Echtzeitgrafik besteht in der Komplexität des Lichtes, welches auf Grund mangelnder
	Rechen- und Speicherkapazitäten nur approximiert werden kann. Die meisten Anwendungen beschränken
	sich auf die Berechnung lokaler Beleuchtungsmodelle.
	Algorithmen für die globale Lichtberechnung werden inzwischen mit GPU Implementierungen Echtzeitfähig,
	als Beispiel soll die Brigade Engine dienen, welche auf dem \textit{Bidirectionalen Pathtracing} Algorithmus
	aufbaut.
	
	Neben dem \textit{Pathtracing} existieren weitere Ansätze, wie das Photonenmapping, welche in der Lage sind
	die Wege des Lichtes, die Verbreitung von Photonen Physikalisch plausibel zu berechnen. 
	Diese Algorithmen kratzen dabei nur an der Oberfläche der Materie. Die Beleuchtungsberechnung wird nur
	an der Materialoberfläche durchgeführt und das Licht wird nicht durch das Volumen gestreut.
	Diese grobe Approximation ist notwendig, da die Volumenintegrale die Rechenzeit ungemein erhöhen.
	
	Datensätze aus der Medizin, der Biologie oder der Meteorologie enthalten oft mehr als die Oberflächenbeschreibung.
	Gemeint sind in diesen Zusammenhang Volumendaten in Form von Skalarfeldern. Vereinfacht gesagt handelt es sich um ein Mehrdimensionales Array von reelen Werten.
	Um ein solches Skalares Feld zu Visualisieren ist es notwendig die Werte zu interpretieren, beispielsweise als
	Dichte eines Voxels.
	Ein Voxel ist in diesem Zusammenhang ein Volumen, die genaue Form spielt dabei eine untergeordnete Rolle.
	Die Abbildung einer solchen Anordnung auf eine Bildebene, wird als Volumenrendering bezeichnet.
	
	Viele Ansätze gehen von einer Richtungslichtquelle aus, welche sich hinter dem Volumen befindet und
	Licht in Richtung der Bildebene der Kamera ausstrahlt. Das Licht wird, auf Grund der Dichte der einzelnen
	Voxel unterschiedlich stark abgeschwächt. Das Ergebnis ist ein Bild des Volumens.
	Es gibt eine Vielzahl von Verfahren zur Berechnung solcher Volumenvisualisierungen, ein oft verwendeter Ansatz
	ist die Ambiente Verdeckung. Hierbei wird geschaut wie viele benachbarte Voxel das Einfallende Licht verdecken,
	aus dieser Anzahl wird der Helligkeitswert des eigentlichen Voxels bestimmt.
	
	In wissenschaftlichen Visualisierungen haben sich neben den Voxelstrukturen auch Kugelglyphen etabliert.
	Hierbei handelt es sich um beliebig große und frei im Raum positionierte Kugeln, die Informationen werden,
	wie bei den Voxelgittern auf die Farbe und die Dichte solcher Kugeln abgebildet.
	Ziel der Visualisierungen ist es die dargestellten Prozesse für die Menschliche Wahrnehmung anschaulich
	abzubilden.
	Durch die Projektion auf ein zweidimensionales Bild ist es nötig die Tiefenwahrnehmung des Menschen mit
	Licht und Schatten zu fördern, da die exakten Positionen der Kugelglyphen sonst nicht erkannt werden können.
	
	Wie bereits erwähnt, lassen sich exakte Verfahren zur Berechnung der Lichtwege durch ein solches Volumen noch
	nicht in Echtzeit berechnen. Approximationen sind nötig um die Geschwindigkeit der Berechnung in Echtzeit zu
	gewährleisten. Die Güte einer solchen Approximation lässt sich anhand von Daten bestimmten, welche nicht
	mit Echtzeitalgorithmen, sondern mit sehr genauen Verfahren gerendert wurden.
	Die entstandenen Bilder werden als \textit{Ground-Truth} Daten bezeichnet. Ziel dieser Arbeit ist es,
	einen Algorithmus zur Erzeugung von \textit{Ground-Truth} Daten zu entwickeln.
	
	Es wird eine Rendergleichung aufgestellt werden, welche weitestgehend analytisch gelöst werden soll.
	Der Vorteil einer Analytischen Lösung besteht in der Exaktheit und in der Geschwindigkeit der Berechnung selbst.
	
\chapter{Verwandte Arbeiten}

	Die Arbeit \cite{JSYR14} wurde verfasst, um den Leser einen Überblick über Techniken des
	renderns von Volumendaten in Echtzeit zu geben.
	Die in dieser Arbeit vorgestellten Algorithmen wurden mit Gütekriterien bewertet und klassifiziert.  
	Diese Kriterien sollen die Auswahl des Verfahrens für den Entwickler erleichtern.
	Alle vorgestellten Modelle sind Approximationen, sie generieren Optisch schöne Bilder, weichen jedoch weit von der Realität ab.
	
	Die Arbeit von Nelson Max \cite{Max:1995:OMD:614258.614298}, beschreibt die Grundlagen des Emission und Absorptionsmodells, welches Grundlage vieler Arbeiten im Bereich des Direkten Volumenrenderns bildet.
	Für die Rendergleichung entwickelt er in dieser Arbeit eine Allgemeine Lösung und beschreibt den Vorgang
	der Lichtstreuung im Detail. Am ende der Arbeit werden einige Lösungsvorschläge zur Implementierung vorgestellt.
	
	In der Publikation \cite{conf/pg/JungPP98} wird ein zum Teil Analytisches Verfahren vorgestellt.
	Der Algorithmus wurde zum Rendern von Voxelgittern konzipiert. 
	Jedem Voxel wird eine Dichte zu geordnet, aus welchem ein Wert für Transparenz abgeleitet wird.
	Hinter dem Voxelgitter befindet sich eine Lichtquelle und deren Lichtintensität wird durch das Volumen unterschiedlich Stark abgeschwächt.	Die Integrale der Gleichungen werden streckenweise Analytisch gelöst,
	wodurch ein Zuwachs an Genauigkeit erreicht wird. 

	\textit{Photon Mapping} ist ein Verfahren zum erzeugen von Globaler Beleuchtung. Es werden Photonen in die
	Szene geschossen, trifft ein Photon die Oberfläche, werden Informationen in der Photonenmap gespeichert.
	Wenn das Photon noch genügend Energie besitzt, wird in Abhängigkeit der Reflektionsrichtung ein weiterer
	Strahl verfolgt. Diese Vorgehensweise wiederholt sich das Photon seine Energie komplett an die Umgebung abgegeben hat, oder ein Abruchkriterium erfüllt wurde.
	In einem zweiten Schritt wird das Bild mit einem Raycast verfahren erzeugt.
	In der Arbeit \cite{JKRY12} wird die Photonenmap mit Histogrammen umgesetzt. Nach Parameteränderungen, werden
	nur die Betroffenen Histogramme ersetzt. Dadurch wird dieses Verfahren Echtzeitfähig.
	
	In der Arbeit \cite{journals/tvcg/AmentSW13} geht es um einen alternativen Ansatz, welcher nicht auf dem Emission und Absorptionsmodell aufbaut, sondern von einem Punkt Stahlen in die umliegende Nachbarschaft verfolgt um den
	Grad an Verdeckung an diesem Punkt zu bestimmen. Je mehr Segmente mit hoher Dichte in der Nachbarschaft liegen,
	umso geringer ist der Anteil des Lichtes, welches eben jenen Punkt erreicht. Ein Geschwindigkeitszuwachs wird durch eine Vor-integrierte Transferfunktion erreicht. Ein Vorteil dieses Verfahrens, stellt die Güte der erzeugten
	Schatten da.

	Eine weitere Arbeit aufbauend auf dem Prinzip der Ambienten Verdeckung stellt die Arbeit \cite{journals/tvcg/KnissPHSM03} vor.
	In dieser Arbeit hat Direktes und Indirektes Licht einen Einfluss auf das Ergebnis. 
	Mit Hilfe des so genannten \textit{blurring} werden die Farbanteile des Indirekten Lichtes in einem Buffer gehalten. Dieses verfahren approximiert die Streuungseffekte, welche in Materialien auftreten können.
	
	Eine Erweiterung dieses Verfahrens stellt die Arbeit \cite{10.1111:j.1467-8659.2009.01464.x} dar, welche hier nur der Vollständigkeit halber erwähnt werden soll.
	
\chapter{Grundlagen}

	Wie in der Einleitung bereits geschildert, wird es in dieser Arbeit ein Raycast Algorithmus beschrieben,
	welcher Lichtdurchlässige Gaskugeln zeichnen wird. In den Folgenden Abschnitten werden die Gleichung
	für den Strahl, die Kugel und die Schnittpunktberechnung genauer betrachtet. Abschließend wird die 
	verwendete Perspektivische Kamera auf der Basis der \textit{Camera obscura} hergeleitet. 
	In der Arbeit werden Punkte im $\mathbb{R}^n$ unterstrichen geschrieben, so symbolisiert $\underline{p}$
	einen Punkt und $\vec{v}$ einen Vektor und die Formel $\langle\vec{a}, \vec{b}\rangle$ bezeichnet ein Skalarprodukt.

\section{Strahl und Kugelgleichung}

	Wie oben beschrieben stellen Strahlen die Elementarsten Elemente eines jedem Stahlenverfolgenden Algorithmus dar.
	Mathematisch lässt sich der Strahl durch die Formel
	
	\begin{equation}
	  	\underline{r}(t) = \underline{p} + t \cdot \vec{r} \text{, mit } t \in \mathbb{R}_+
	  	\label{eq:Strahl}
	\end{equation}
	
	beschreiben. Jeder Punkt auf dem Strahl $\underline{r}(t)$, ergibt sich aus der Addition eines Stützpunktes $\underline{p}$ mit dem durch $t$ skalierten Richtungsvektor $\vec{r}$.
	
	Eine Kugelfläche wird im $\mathbb{R}^3$ mit dem Mittelpunkt $\underline{m} = (x_0, y_0, z_0)$ und dem Radius $r$ parametrisiert. Durch den Abstand zum Mittelpunkt lässt sich jeder Punkt auf der Kugel mit der Formel

	\begin{equation}
		(x - x_0)^2 + (y - y_0)^2 + (z - z_0)^2 = r^2
		\label{eq:KugelGleichung1}
	\end{equation}
	
	bestimmen. Jeder Punkt $\underline{p} = (x, y, z)$ der die Formel \ref{eq:KugelGleichung1} erfüllt liegt auf der Oberfläche der Kugel.
	Die Formel \ref{eq:KugelGleichung1} lässt sich auf beliebige $\mathbb{R}^n\text{, mit } n \in \mathbb{N}_+$ erweitern:
	
	\begin{equation}
		\langle\underline{p} - \underline{m}, \underline{p} - \underline{m}\rangle = r^2.
		\label{eq:KugelGleichung2}
	\end{equation}

	Wird der Mittelpunkt der Kugel auf den Koordinaten Ursprung verschoben, liegen alle Punkte $p$ auf der Oberfläche $O$, wenn sie denn Abstand $r$ zum Koordinatenursprung haben. Die zu erfüllende Bedingung hat demnach folgende Form:
	
	\begin{equation}
		\vert\vert\underline{p}\vert\vert = r
		\label{eq:KugelGleichungBedingun1}
	\end{equation}
	
	Die Bedingung aus Formel \ref{eq:KugelGleichungBedingun1} lässt sich Quartieren wodurch sich die folgende Kugelgleichung ergibt:
	
	\begin{equation}
		\langle\underline{p}, \underline{p}\rangle = r^2
		\label{eq:Schnittpunktberechnung1}
	\end{equation}
	
	Um den Schnittpunkt der Kugel mit dem Strahl aus der Formel \ref{eq:Strahl} zu Berechnen, wird der Strahl in die Gleichung \ref{eq:Schnittpunktberechnung1} eingesetzt
	
	\begin{equation}
			r^2 = \langle r(t), r(t) \rangle = \langle \underline{p} + t \cdot \vec{r}, \underline{p} + t \cdot \vec{r} \rangle
			\label{eq:Schnittpunktberechnung3}
	\end{equation}
	
	und nach dem Parameter $t$ aufgelöst. Die Gleichung ist Quadratisch, daher ergeben sich entweder keine,
	eine oder zwei Lösungen für die Gleichung:

	
	\begin{equation}
		t_{1,2} = \frac{-2 \cdot \langle \underline{p}, \vec{r} \rangle \pm \sqrt{4 \cdot \langle \underline{p}, \vec{r} \rangle^2 - 4 \cdot \langle \underline{p}, \underline{p} \rangle \cdot \langle \vec{r}, \vec{r} \rangle}}{2 \cdot \langle \vec{r}, \vec{r} \rangle}.
		\label{eq:Schnittpunktberechnung5}
	\end{equation}

\section{Die Perspektivische Kamera}

	Um Bilder zu erzeugen wird eine Kamera, mit einer Bildebene benötigt, auf welcher die Lichtintensitäten akkumuliert werden. Es soll eine Perspektivische Kamera verwendet werden, welche sich von
	der im folgenden beschriebenen \textit{Camera obscura} ableitet.
	
	Eine \textit{Camera obscura} besteht aus einem Kasten, auf der einen Seite befindet sich ein schmales Loch,
	durch welches das Licht einer beleuchteten Szene auf die gegenüberliegende Wand trifft, diese
	Wand wird als Bildebene Bezeichnet. 
	Auf der Bildebene entsteht dabei ein auf dem Kopf stehendes und seitenverkehrtes Bild der Szene.
	
	\begin{figure}[h]
		\centering
		\includegraphics{images/cameraObscura.pdf}
		\caption{Schematische Darstellung des Abbildungsvorgangs der \textit{camera obscura}}
		\label{fig:cameraObscura}
	\end{figure}
	
	Die Abbildung \ref{fig:cameraObscura} zeigt den Aufbau der \textit{Camera obscura} schematisch.
	Auf der Linken Seite der Abbildung befindet sich die Bildebene, in der Mitte die Wand mit dem kleinen Loch.
	Rechts befindet sich das mit den Punkten $A$ und $B$ abzubildende Objekt. Der Strahl welcher von dem Punkt $B'$
	nach $B$ zeigt definiert die Blickrichtung der Kamera. Der Punkt $A$ wird auf den Punkt $A'$ abgebildet und
	der Punkt $B$ auf den Punkt $B'$. Die Distanz zwischen der Bildebene und dem Loch der Kamera wird mit der
	Variable $f$ bezeichnet.
	Ein mit diesem Modell generiertes Bild müsste nach der Aufnahme gedreht werden. In der Virtuellen Welt,
	lässt sich Bildebene Vor dem Loch positionieren. Die Position $\underline{p}$ der Kamera ist mit der Position
	des Lochs der \textit{Camera obscura} identisch.
	Für jeden Pixel der Bildebene wird jetzt mindestens ein Strahl generiert welcher als Stützpunkt die Position
	der Kamera erhält und der Richtungsvektor ist der Normierte Vektor von der Kamerapostion zum Pixel. 
	
	Die Sensorfläche der Kamera setzt sich aus $W \cdot H$ Pixeln zusammen. Das Verhältnis zwischen Höhe und Breite wird im Englischen als \textit{aspect ratio} bezeichnet und definiert sich als Quotient aus der Breite und Höhe $aspect = \frac{W}{H}$. Der Richtungsvektor eines Pixel an der Stelle $(x, y)$ auf der Bildebene definiert sich
	durch die Formel:
	
	\begin{equation}
		\vec{r}(x, y) = \left|\left|\left( 
							\begin{array}{l}
								(\frac{x}{W} \cdot 2 - 1) \cdot aspect \\
								\frac{y}{H} \cdot (- 2) + 1 \\
								f
							\end{array}							
					 	\right)\right|\right|.
		\label{eq:direction}
	\end{equation}
	
	Für jeden Pixel $(x, y)$ wird ein Strahl $s$ in die Szene geschossen mit der Formel
	$\underline{s} (x, y, z) = \underline{p} + t \cdot \vec{r}(x, y)$. Jetzt lässt sich die Kamera frei positionieren
	und der Zoom mit dem Parameter $f$ bestimmen. Zu diesen Zeitpunkt ist es nur möglich in Richtung der Positiven $z$-Achse zu schauen. Um die Kamera zu rotieren genügt es die Richtungsvektoren $\vec{r}(x, y)$ mit Hilfe
	einer Rotationsmatrix um den Koordinaten Ursprung zu rotieren.
	
\chapter{Ein analytischer Ansatz}
 
\section{Paradigma}

	In diesem Kapitel wird der Algorithmus hergeleitet welcher zum Zeichnen der Kugeln dient.
	Zum Beginn wird das Emission und Absorptionsmodell aus der Arbeit von \cite{Max:1995:OMD:614258.614298}
	genauer betrachtet. Als Grundannahme wird die Menge der Transferfunktionen auf lineare Funktionen beschränkt,
	die Dichte soll im Volumen als Konstant gelten. Diese Arbeit befasst sich mit der Frage, ob sich mit
	diesen Einschränkungen das entstehende Volumenintegral analytisch lösen lässt.
	Gibt es eine analytische Lösung, so würden sich die Bilder sehr effizient und korrekt berechnen lassen.
	Lässt sich Gleichung nicht analytisch Lösen, werden die Integrale so weit wie möglich analytisch gelöst
	und die restlichen Teile müssen Nummerisch approximiert werden.
	
\section{Das Optische Modell}

	Wie im vorherigen Abschnitt beschrieben baut das Modell auf dem von Nelsen Max aus der Arbeit \cite{Max:1995:OMD:614258.614298} auf. Zur Herleitung seiner Formel betrachtete er Partikel in Form
	von Einheitskugeln. Jede dieser Kugeln besitzt einen Radius $r$. Die Oberfläche eines Partikel $A$
	bestimmt sich durch das Produkt der Kreiszahl $\pi$ und dem Radius. So das gilt $A = \pi \cdot r$.
	Die Mittlere Dichte $\rho$ entspricht der Anzahl von Partikeln, welche in einem Einheitsvolumen zu finden sind.
	Betrachtet wird im folgenden ein Zylinder, welcher mit der Kreisfläche $E$ und einer Länge $\Delta s$ parametrisiert wird. 
	Das Volumen eines solchen Zylinders entspricht $V_z = E \cdot \delta s$ und es enthält in etwa $N$ Partikel, mit $N = \rho E \delta s$.
	Die von dem Zylinder verdeckte Grundfläche $B$ entspricht bei einem sehr klein gewählten $\delta s$ in etwa $NA$, mit $NA = \rho AE \delta s$.
	Als Flussrichtung des Lichtes wird $\delta s$ gewählt. Der Anteil des Lichts, welcher mit Teilchen Wechselwirkt bis er $B$ erreicht beträgt $\rho A \delta s$. 
	Wenn $\delta s$ gegen Null geht, sinkt die Wahrscheinlichkeit, das sich Gaspartikel entlang der Lichtrichtung überlappen. Die Funktion $I(s)$ liefert die Intensität des Lichtes an der Distanz $s$. Wird die Lichtintensität
	$I(s)$ nach $s$ abgeleitet, ergibt sich die folgender Differenzialgleichung:
		 
	\begin{equation}
		\frac{dI}{ds} = -\rho(s)AI(s) = -\tau(s)I(s)
		\label{eq:MAX95grundDG}
	\end{equation}
	
	Auf der rechten Seite der Gleichung \ref{eq:MAX95LSGgrundDG} steht die Funktion $\tau(s)$, diese beschreibt
	die Abschwächung der Lichtintensität durch ein Volumen der Länge $s$. Wie in \ref{eq:MAX95EmissionDG} zusehen
	ist definiert sich der Abschwächungskoeffizient als das negative Produkt der mittleren Dichte $\rho$ mit der
	Oberfläche eines Partikels. Nelson Max hat die folgende Lösung für die Differenzialgleichung gefunden:
	
	\begin{equation}
		I(s) = I_0 \cdot e^{- \int\limits_{0}^{s} \tau(t) dt}
		\label{eq:MAX95LSGgrundDG}
	\end{equation}
	
	Der Parameter $I_0$ entspricht der Intensität an der Stelle $s = 0$, dabei handelt es sich um den Punkt,
	an dem der Lichtstrahl auf das Volumen trifft. Bei den zweiten Teil des Terms handelt es sich um die Transparenz
	des Mediums im Intervall $[0, s]$, welche durch $T(s) = exp(- \int_{0}^{l} \tau(t) dt)$ repräsentiert wird. 
	Nelson Max definiert neben der Transparenz einen Wert für den Grad an
	Verdeckung durch das Volumen $\alpha$, der im Englischen als \textit{opacity} bekannt ist.
	
	\begin{equation}
		\alpha = 1 - T(l) = 1 - e^{- \int\limits_{0}^{l} \tau(t) dt}
		\label{eq:MAX95Opacity}
	\end{equation}
	
	Ist die Funktion $\tau$ innerhalb des Volumens konstant, vereinfacht sich der Term für die Verdeckung zur Gleichung $\alpha = 1 - exp(-\tau l) = \tau l - (\tau l)^2 / 2 + \cdots$. 
	Im Rahmen dieser Arbeit ist der Begriff der Transferfunktion bereit mehr als einmal gefallen,
	eine solche Funktion bildet den Materialwert auf die optischen Eigenschaften der Gleichung ab.
	
\subsection{Emssion}

	Wie in der Arbeit \cite{Max:1995:OMD:614258.614298} wird auch in dieser zuerst die Gleichung für die Emission
	hergeleitet. Neben der Abschwächung der Lichtintensität durch ein Volumen, kann zusätzlich an jedem Punkt in diesem Licht emittiert werden. In Worten bedeutete es, das ein Lichtstrahl welcher durch das Volumen geschossen
	wurde zusätzlich mit Licht angereichert wird. Zur Herleitung der Emission soll die Absorption zu nächst vernachlässigt werden. Betrachtet werden die Partikel aus dem vorhergehenden Abschnitt, jeder dieser Partikel
	wird im Folgenden als Transparent angenommen. Zusätzlich emittiert jeder von ihnen diffuses Licht. Das bedeutet
	jeder Partikel emittiert Licht, in alle Richtungen mit der gleichen Intensität $C$ über der projizierten Fläche $\rho A E \Delta s$. Dieser Effekt bewirkt eine Anreicherung des Lichtfluss $C \rho A E \Delta s$ welcher zur Basisfläche $E$ fließt.
	Durch diesen Zusammenhang ergibt sich eine weitere Differenzialgleichung:
	
	\begin{equation}
		\frac{dI}{ds} = C(s)\rho(s)A = C(s)\tau(s) = g(s)
		\label{eq:MAX95EmissionDG}
	\end{equation}
	
	Die Funktion $g(s)$ wird als Quellterm bezeichnet. Dieser Term beschreibt die Wechselwirkung des Lichtes mit dem
	Volumen über der Länge $s$. Zu der Differenzialgleichung \ref{eq:MAX95EmissionDG} hat Nelson Max ebenfalls eine
	Lösung gefunden welche sich wie folgt definiert:
	
	\begin{equation}
			I(s) = I_0 + \int\limits_{0}^{s} g(t) dt
			\label{eq:MAX95LSGEmissionDG}
	\end{equation}
	
	Ein Problem dieser Lösung ist die Abhängigkeit zwischen der Ausdehnung des Mediums und des Emissionsfaktors.
	Es gibt keine obere Schranke, das bedeutet, das die Emittierte Lichtenergie sehr schnell die Szene mit Licht
	überfluten kann und die Abbildung der Materialparameter in Abhängigkeit der Objektgrößen gewählt werden müssen.
	
	
\section{Optical Models for Direct Volume Rendering}
	
\subsection{Emission}

	
\subsection{Absorption und Emission}

	In der Vorhergehenden Abschnitt wurden die Volumen als vollkommen Transparent betrachtet, dieses Verhalten soll sich jetzt ändern.
	Das Licht, welches das Volumen durchläuft wird soll mit Hilfe des Abschwächungskoeffizient abgeschwächt werden und zusätzlich durch
	die Emission angereichert werden:
	
	\begin{equation}
		\frac{dI}{ds} = g(s) - \tau(s)I(s)
		\label{eq:MAX95EmissionAbsorptionDG}
	\end{equation}
	
	In seiner Arbeit entwickelt Nelson Max eine Lösung für beliebige Quellfunktionen, welche $g(s)$ annehmen kann. 
	Dabei wird das Licht als Strahl von der Lichtquelle durch das Medium geschickt. 
	Die Variable $s$ entspricht dem Laufparameter des Strahls,
	welcher an der Position der Lichtquelle den Wert Null annimmt und den Wert $D$ am Sensor hat.
	
	\begin{equation}
		I(D) = I_0 \cdot e^{-\int\limits_{0}^{D}\tau(t)dt} + \int\limits_{0}^{D}g(s) \cdot e^{-\int\limits_{s}^{D}\tau(t)dt}ds
		\label{eq:MAX95LSGEmissionAbsorptionDG1}
	\end{equation}
	
	\begin{equation}
		I(D) = I_0T(D) + \int\limits_{0}^{D}g(s)T'(s)ds
		\label{eq:MAX95LSGEmissionAbsorptionDG2}
	\end{equation}
	
	Wobei die Transparenz über die einzelnen Werte von s sich als $T'(s) = e^{-\int\limits_{s}^{D}\tau(x)dx}$ definiert. Das Integral aus der Gleichung \ref{eq:MAX95LSGEmissionAbsorptionDG1} lässt sich für einige Transferfunktionen analytisch lösen für die anderen schlägt Nelson Max die
	Approximation durch Riemannsummen vor. Ist der Faktor $C(s)$ eine Konstante $C$, lässt sich die Lösung stark vereinfachen. und das Licht,
	welches den Sensor erreicht ergibt sich durch die Gleichung:
	
	\begin{equation}
		I(D) = I_0 T(D) + C(1 - T(D))
		\label{eq:MAX95LSGEmissionAbsorptionWithCKonstDG1}
	\end{equation}
	
	Die Lichtundurchlässigkeit $\alpha = (1 - T(D))$ und entspricht der Wahrscheinlichkeit, dass ein Strahl, welcher vom Sensor in Richtung der Lichtquelle
	geschossen wird einen Partikel trifft und die Farbe C zu sehen ist. 
	
\subsection{Streuung und Schatten}

	Die bisher vorgestellten Gleichungen erzeugen bereits schöne Bilder, entsprechen allerdings noch nicht der Realität. Licht welches auf ein Partikel
	trifft wird gestreut, es entstehen Schatten. Nelson Max geht dabei auf die Grundlagen der Streuung ein, in dem der Quellterm $g(s)$ um einen Winkelparameter erweitert wird. 
	Der Quellterm wird im Folgenden mit $g(X, \omega) = E(X) + S(X, \omega)$. 
	$E(X)$ bezeichnet die Emission an einen bestimmten Punkt $X$ und $S(X, \omega)$ die Richtung in welche das Licht gestreut wird. Trifft das Licht ein Partikel an einem Punkt $X$, wird dessen Intensität
	mit einer \textit{brdf} Funktion multipliziert. \textit{Brdf} steht für 
	\textit{bidirectional reflection distribution function}, welche die Verteilung des Lichtes in Abhängigkeit
	des Einfalls und Ausfallwinkels an dem Punkt $X$ bestimmt. 
	Die Streufunktion $S(X, \omega) = r(X, \omega, \omega') \cdot i(X, \omega)$ setzt sich zusammen aus der
	\textit{brdf} welche hier durch die Funktion $r$ dargestellt wird und der Intensität des Lichtes $i$,
	welches aus der Winkelrichtung $\omega'$ in den Punkt $X$ einfällt. Mit Hilfe eines rekursiven \textit{Raytracers} kann der Lichtanteil reduziert werden, was einen Schattenwurf zur Folge hat.
	
\section{An Analytical Ray Casting of Volume Data}

	Diese Arbeit baut auf der Grundlage von CT, MRI, SPECT und PET Datenmengen auf. Das bedeutet, alles was
	vorliegt ist ein 3D-Array von skalaren Werten, welche im Folgenden als Dichte interpretiert werden.
	Das hier vorgestellte Verfahren baut auf einen \textit{Raycaster} auf, das heißt es werden ausgehend vom
	Bildsensor Stahlen in die Szene geschossen. Das Resultat ist ein Bild, welches durch die Abschwächung der
	Hintergrundbeleuchtung durch das Volumen entsteht. Durch den Einsatz von Interaktionstechniken, kann
	der Nutzer im Folgenden Teile des Volumen durch die Veränderung der Dichtewerte ausblenden oder hervorheben.
	Die meisten \textit{Raycast}-Ansätze approximieren diesen Vorgang nummerisch, da das finden, von Analytischen
	Lösungen, insofern möglich nicht einfach ist. In dieser Arbeit wird ein Verfahren vorgestellt,
	welches zum Teil die Aufkommenden Integrale Analytisch löst und wenn nötig auf Numerische Praktiken zurückgreift.
	
\subsection{Levoy's raycast algorithm}

	Grundlage dieser Arbeit ist ein \textit{Raycast}-Algorithmus von Levoy. Die betrachtete Welt besteht aus
	einem Regulären Gitter, wobei jede Zelle ein Voxel darstellt. Jeder dieser Voxel hat 8 Nachbarn.
	Im folgenden beschreibt $r$ einen Strahl, welcher in die Szene geschossen wird. Jeder Punkt entlang
	des Strahls lässt sich über die Länge $u$ vom Beginn des Strahls bis zu dem entsprechenden Punkt bestimmen.
	Zusätzlich wird eine Funktion $\alpha_C(u)$ definiert, welche die akkumulierten Verdeckungen vom Punkt an der
	Stelle $u$ bis zum Pixel liefert. Diese Funktion repräsentiert die Abschwächung des Lichtes durch das Volumen.
	$\Delta u$ definiert eine bestimmte Länge auf dem Strahl zwischen zwei benachbarten Stützpunkten.
	Ein Stützpunkt wird in dieser Arbeit mit $u_i$ bezeichnet. Die Abschwächung des Lichtes an einem
	gewählten Stützpunkten wird über die Formel $\alpha(u_i) \Delta u_i$ berechnet. Wie in der Arbeit von
	Nelson Max berechnet sich die Transparenz durch $1 - \alpha_C(u)$. 
	$u + \Delta u$ ist ein Punkt mit der Transparenz $1 - \alpha_C(u + \Delta u)$. Die Transparenz
	zwischen den Punkt $u$ und $u + \Delta u$ wird mit $1 - \alpha(u) \Delta u$ angegeben.
	Die Transparenz zwischen dem Pixel und dem Punkt $u + \Delta u$ setzt sich folgendermaßen zusammen:
	
	\begin{equation}
		1 - \alpha_C(u + \Delta u) = (1 - \alpha_C(u)) \cdot (1 - \alpha(u)\Delta u)
		\label{eq:TransparenzGL1}
	\end{equation}
	
	Daraus ergibt sich eine rekursive Beziehung für die aufakkumulierten Verdeckungen für $\alpha_C(u)$
	
	\begin{equation}
		\alpha_C(u + \Delta u) = \alpha_C(u) + \alpha(u)(1 - \alpha_C(u)) \Delta u
		\label{eq:TransparenzGL2}
	\end{equation}
	
	Bezeichnet $C_C(u)$ die akkumulierte Farbe, lässt diese sich mit einer ähnlichen rekursiven Formel
	berechnen. $C(u)$ stellt die Farbe dar, welche von einem Punkt, mit dem Längenparameter $u$ durch
	ein vollkommen transparentes Volumen auf den Pixel Reflektiert wird. Zur Berechnung der Farbintensität
	wird das Phong Beleuchtungsmodell benutzt. In der Realität ist das Medium zwischen dem Punk an der Stelle $u$
	und dem Pixel selten vollkommen Transparent, deshalb muss die Farbe durch das Volumen abgeschwächt werden.
	$C(u) \alpha(u) \Delta u$ beschreibt diese Abschwächung.
	
	\begin{equation}
		C_C(u + \Delta u) = C_C(u) + (1 - \alpha_C(u)) \cdot C(u) \alpha(u) \Delta u
		\label{eq:TransparenzGL3}
	\end{equation}
	
\subsection{Eine kontinuierliche Formulierung}

\section{Verdeckung}

Die endgültige Lichtintensität, die das Auge erreicht setzt sich zusammen aus:
 
$I = I_E + I_B + I_L$

dem emittierten Licht:

$I_E = \int\limits_{t_n}^{t_f} \tau(t_n, t) \cdot c(v_r(t)) dt $

dem ambienten Licht:

$I_B = \tau(t_n, t_f) \cdot I_b$

$\tau(t_0, t_1) = e^{- \int\limits_{t_0}^{t_1} \sigma (v_r(t)) dt}$

und dem Licht der Einzelnen Lichtquellen. Hier reduziert auf einen einzelnen Strahl der den Sichtstrahl $\underline{r}(t) = \underline{e} + t \cdot \vec{v}$ im Volumen schneidet. (An der Stelle t)

$I_L(t) = \tau(t_n, t) \cdot I_p(t)$

$I_p(t) = e^{- \int\limits_{0}^{l(t)} \cdot  \sigma (v_r(s)) ds} \cdot I_p$

$I_p(t) = e^{- \int\limits_{0}^{l(t)} \cdot  \lambda \cdot v_r(s) ds} \cdot I_p$

$I_p(t) = e^{- \int\limits_{0}^{l(t)} \cdot  \lambda \cdot \kappa ds} \cdot I_p$

$I_p(t) = e^{- \int\limits_{0}^{l(t)} \cdot  \lambda \cdot \kappa ds} \cdot I_p$

$I_p(t) = e^{- \lambda \cdot \kappa \cdot \int\limits_{0}^{l(t)} ds} \cdot I_p$

$I_p(t) = e^{- \lambda \cdot \kappa \cdot s} \cdot I_p$

$I_p(t) = e^{- \lambda \cdot \kappa \cdot \frac{-<p(t), d(t)>}{<d(t), d(t)>} + \frac{\sqrt{<p(t), d(t)>^2 - <p(t), p(t)> \cdot <d(t), d(t)>}}{<d(t), d(t)>}} \cdot I_p$

mit $d(t) = d$ da es sich um eine Richtungslichtquelle handelt

$I_p(t) = e^{- \lambda \cdot \kappa \cdot \frac{-<p(t), d>}{<d, d>} + \frac{\sqrt{<p(t), d>^2 - <p(t), p(t)> \cdot <d, d>}}{<d, d>}} \cdot I_p$

$I_{L(t)} = \tau(t_n, t) \cdot e^{- \lambda \cdot \kappa \cdot \frac{-<p(t), d>}{<d, d>} + \frac{\sqrt{<p(t), d>^2 - <p(t), p(t)> \cdot <d, d>}}{<d, d>}} \cdot I_p$

Daraus ergibt sich $I_L$ für eine Richtungslichtquelle

$I_L = \int\limits_{t_n}^{t_f} I_{L(t)} dt $

$I_L = \int\limits_{t_n}^{t_f} \tau(t_n, t) \cdot I_{p(t)} dt$

$I_L = \int\limits_{t_n}^{t_f} \tau(t_n, t) \cdot e^{- \lambda \cdot \kappa \cdot \frac{-<p(t), d>}{<d, d>} + \frac{\sqrt{<p(t), d>^2 - <p(t), p(t)> \cdot <d, d>}}{<d, d>}} \cdot I_p dt$


\cite{Max:1995:OMD:614258.614298}

\cite{conf/pg/JungPP98}

\chapter{Fazit}

%\chapter{title}
%
%\chapter{ein kapitel}
%\section{eine Grafik}
%\begin{figure}[htbp]
%	\centering
%		\includegraphics{test.png}
%	\caption{beschriftung}
%	\label{fig:diplominf}
%\end{figure}

\end{document}