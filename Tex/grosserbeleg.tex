\documentclass[hyperref,german,beleg,final,twoside]{cgvpub}
%weitere Optionen zum Ergänzen (in eckigen Klammern):
% 
% female	weibliche Titelbezeichnung bei Diplom
% bibnum	numerische Literaturschlüssel
% final 	für Abgabe	
% lof			Abbildungsverzeichis
% lot			Tabellenverzeichnis
% noproblem	keine Aufgabenstellung
% notoc			kein Inhaltsverzeichnis
% twoside		zweiseitig

\usepackage{tabularx}

\author{Josef Schulz}
\title{Ground-Truth-Renderer für Partikelbasierte Daten}
\birthday{20. Oktober 1989}
\placeofbirth{Dresden}
\matno{3658867}

\betreuer{Dipl-MedienInf. Joachim Staib}
\bibfiles{literatur.bib}
\problem{
Die Darstellung von Partikeldaten mittels Kugelglyphen ist in wissenschaftlichen Visualisierung
inzwischen etabliert. Gerade bei dichten Datensätzen stellen kompakte Anordnungen von sehr vielen
Kugeln jedoch ein Problem für die Erkennbarkeit der zu visualisierenden Vorgänge dar. Eine Möglichkeit, diesem Problem zu begegnen ist es, über Blinn-Phong-Beleuchtung hinausgehende Effekte wie
globale Schatten oder den Einsatz von Methoden aus dem Volume-Rendering zu integrieren. Durch
deren Komplexität muss in Echtzeitvisualisierungen jedoch auf teilweise grobe Approximationen zu-
rückgegriffen werden. Die Einschätzung der Approximationsqualität fällt häufig schwer, da keine Visualisierung des exakten Verfahrens verfügbar ist.
Ziel dieser Belegarbeit ist die Umsetzung eines CPU-Renderers für Partikeldaten, der eine Reihe
von erweiterten Visualisierungseffekten unterstützt. Er soll die Grundlage für Ground-Truth-
Visualisierungen bieten.
Zunächst soll eine geeignete Softwarearchitektur konzipiert und umgesetzt werden. Die Partikel sollen als mit lichtemittierendem und ?absorbierendem Gas gefüllte Kugeln interpretiert werden. Es sollen anschließend Methoden entwickelt werden, um einen physikalisch plausiblen globalen Schattenwurf und Lichttransport für eine beliebige Anzahl an Punkt- und Richtungslichtquellen zu ermöglichen.
Die dafür notwendigen Gleichungen für Kugeln mit konstanter Dichte und Emission, sowie linearer
Absorption, sollen soweit wie möglich analytisch bestimmt und, sobald nicht mehr möglich, mittels
möglichst exakter numerischer Integratoren ausgewertet werden.

Die Teilaufgaben umfassen:

\begin{itemize}
\item Umfassende Literaturrecherche zur globalen Beleuchtungsrechnung in der Volumen Visualisierung
\item Schrittweise Konzeption und Umsetzung einer erweiterbaren Architektur zum Erzeugen von Ground-Truth-Bildern:

	\begin{enumerate}
		\item Zunächst als Raytracer für opake Kugeln, der globale Schatteneffekte von frei
		positionierbaren Punkt- und Richtungslichtquellen unterstützt
		\item Umsetzung eines Renderers, der Kugeln als Volumen nach dem Emissions-Absorptions-Modell rendert, dabei analytische Bestimmung des Volume-Rendering-Integrals, einschließlich Integration direkter Beleuchtung unverdeckter Lichtquellen
		\item Erweiterung zu verdeckten Lichtquellen und Bestimmung der Lichtstärke- und Farbe
		für Lichtstrahlen durch verdeckende Kugeln
	\end{enumerate}
\item Unterstützung für ein Standardformat wie VRML
\item Evaluation in Bezug auf Korrektheit, Bildartefakte und (numerische) Grenzfälle
\end{itemize}
\newpage
Optional:
\begin{itemize}
\item Unterstützung für Refraktionseffekte
\item Unterstützung komplexerer Materialtypen
\end{itemize}
}

\copyrighterklaerung{Hier soll jeder Autor die von ihm eingeholten
Zustimmungen der Copyright-Besitzer angeben bzw. die in Web Press
Rooms angegebenen generellen Konditionen seiner Text- und
Bild"ubernahmen zitieren.}

\acknowledgments{Die Danksagung...}
	
	 
\begin{document}

\chapter{Einleitung}

\section{Motivation}
	
	Bei einer Vielzahl von Messvorgängen oder Simulationen ergeben sich Partikel basierte Datensätze.
	Erst durch eine geeignete Darstellung lassen sich diese optimal auswerten und analysieren.
	Das Ziel von Visualisierungen ist die Unterstützung der menschlichen Wahrnehmung bei der Auswertung
	der erhobenen Daten.
	Dabei geht es nicht um eine bloße Optische Verschönerung der Darstellung, sondern darum kodierte Eigenschaften 
	in den Fokus des Betrachter zu rücken und Tendenzen hervorzuheben.
	Die Darstellung von dreidimensionalen Partikeln auf eine zweidimensionalen Ebene ist mit Problemen verbunden,
	diese sollen im Folgenden näher betrachtet und werden.
	
	Die Fähigkeit des Menschen die Tiefe von Objekten in einer Szene schätzen zu können wird durch ein System ermöglicht,
	das den Wert für die Tiefe auf unterschiedlichen Wegen bestimmt. Ein Weg besteht in der Schätzung der Entfernung über
	die Epipolargeometrie. Bei der Betrachtung eines zweidimensionalen Bildes kann diese Variante lediglich die Bildschirmebene erfassen, jedoch nicht die Tiefe der abgebildeten Objekte. Die Objekttiefen in der Abbildung
	kann der Mensch anhand von Überdeckungen und durch eine perspektivische Transformation der Objekte abschätzen.
	Damit Überdeckungen erkannt und die abgebildeten Objekte von einander unterschieden werden können müssen sich diese
	sich durch einen Kontrast voneinander abheben, Schattierungen lösen dieses Problem. Zusätzlich wirkt ein Kugelglyph plastischer als wenn er durch einen einfarbiger Kreis darstellen werden würde. 	
	Überdeckungen verhindern allerdings, das der Betrachte die dahinter liegenden Partikel sehen kann. 
	Ganze Strukturformen zu denen beispielsweise Höhlen zählen können deshalb nicht erfasst werden.
	Eine Möglichkeit dieses Problem zu lösen besteht darin die Partikel transparent abzubilden, damit trotz
	der Überdeckung alle Strukturen sichtbar bleiben.
	Die zuvor erwähnten Schattierungen ergeben sich in der realen Welt aus der Wechselwirkung zwischen Licht und
	Materie. Ein weiter Weg zur Bestimmung von Objekttiefen die der Mensch nutzt, ist die Rekonstruktion von
	Lichtwegen. 
	
	Lokale Beleuchtungsmodelle eigen sich zur Berechnung von Schattierung, unterstützen die Wahrnehmung der Tiefe jedoch nur bedingt.
	Es existieren eine reihe von Beleuchtungsmodellen die die Szene global Beleuchten. 
	Der Einsatz solcher Modelle stellt eine bessere Unterstützung der Wahrnehmung dar.
	Existierenden Algorithmen müssen auf Grund einer hohe Rechenkomplexität der globalen Beleuchtung an vielen stellen approximativ vereinfachen.
	Insbesondere GPU-Implementierung sind auf diese angewiesen um die globale Beleuchtung in Echtzeit zur realisieren.
	
	Ziel dieser Arbeit ist die Entwicklung eines CPU-Renderes für Partikel, bei dem der Fokus auf der Genauigkeit und nicht auf der Geschwindigkeit der Berechnung liegt.
	Die Partikel werden als mit Gas gefüllte Kugel interpretiert und mit einem Verfahren der direkten Volumen Darstellung
	gezeichnet. Direkte Verfahren zerlegen die Daten nicht in einem Vorverarbeitungsschritt in Netze aus Polygonen,
	sondern visualisieren die Daten in einem Schritt.
	Die Grundlage des Verfahrens stellt ein Volumenintegral dar, dessen Lösung mit Hilfe eines Raycast-Algorithmus
	dargestellt wird. Dabei handelt es sich um ein halb-analytisches Verfahren.
	Vorteile analytischer Lösungen in der Genauigkeit und der Geschwindigkeit der Berechnung.  
	Die erzeugten Bilder, dienen als \textit{Ground-Truth} Information zur Evaluation von approximativen Implementierungen. 
	
\section{Struktur der Arbeit}
	
	Im Folgenden werden Verwandte Arbeiten vorgestellt gefolgt von den Grundlagen des Raycast-Verfahrens, das nach
	der Einführung von Strahl und Kugelgleichungen mit der Herleitung der Perspektivischen Kamera schließt.
	Das vierte Kapitel stellt das verwendete Beleuchtungsmodell vor. Details zur Implementierung sind im fünften Kapitel
	zu finden. Beendet wird die Arbeit mit einer Auswertung des Verfahren in Abhängigkeit gewählter Parameter.
	
\chapter{Verwandte Arbeiten}

	Die Arbeit \cite{JSYR14} fast verschiedenen Algorithmen zur Beleuchtung und interaktiven Darstellung
	von Volumendaten in Echtzeit zusammen.
	Vorgestellte Algorithmen werden hinsichtlich ihrer technischen Umsetzung, Performance und ihrer
	Unterstützung für die menschliche Wahrnehmung klassifiziert.  
	Diese Kriterien erleichtern die Auswahl des Verfahrens für den Entwickler in Abhängigkeit der Anwendung.
	Bei den Verfahren handelt es sich um approximative Varianten mit dem Ziel interaktive Bildraten zu
	gewährleisten.
	
	Die Grundlagen des Emission-Absorption-Modells werden von Nelson Max in der Arbeit \cite{Max:1995:OMD:614258.614298} beschrieben.
	Das Volumenintegral des Emission-Absorption-Modells wird hergeleitet und Stück für Stück mit Effekten
	angereichert. Neben der einfachen Streuung und der Schattenberechnung wird auch die Mehrfachstreuung vorgestellt.
	Beendet wir die Arbeit mit der Vorstellung anwendbarer Lösungsverfahren.
	
	In der Publikation \cite{conf/pg/JungPP98} wird ein zum Teil Analytisches Verfahren vorgestellt. 
	Der Algorithmus wird zur Darstellung von Voxelgittern eingesetzt.  
	Jedem Voxel wird eine Dichte zu geordnet, aus welcher ein Wert für die Transparenz abgeleitet wird.
	Hinter dem Voxelgitter befindet sich eine Lichtquelle und deren Lichtintensität wird durch das Volumen  unterschiedlich Stark abgeschwächt. 
	Die Integrale der Gleichungen werden streckenweise Analytisch gelöst, wodurch ein Zuwachs an 
	Genauigkeit erreicht wird. 
	
	In der Arbeit \cite{journals/tvcg/AmentSW13} geht es um einen alternativen Ansatz, welcher nicht auf dem Emission und Absorptionsmodell aufbaut, sondern von einem Punkt Stahlen in die umliegende Nachbarschaft verfolgt um den
	Grad an Verdeckung an diesem Punkt zu bestimmen. Je mehr Segmente mit hoher Dichte in der Nachbarschaft liegen,
	umso geringer ist der Anteil des Lichtes, welches eben jenen Punkt erreicht. Ein Geschwindigkeitszuwachs wird durch eine Vor-integrierte Transferfunktion erreicht. Ein Vorteil dieses Verfahrens, stellt die Güte der erzeugten
	Schatten da.

	Photonen Mapping ist ein Verfahren zur Erzeugung von Globalen Beleuchtungseffekten, dieses kommt
	in der Arbeit \cite{JKRY12} zur Beleuchtung von Volumendaten in Echtzeit zum Einsatz.
	Die benötigte dazu benötigte Photonenmap wir dabei mit Histogrammen realisiert. 
	Mit deren Hilfe können Änderungen in Interaktiven Bildwiederholungsraten ermöglicht werden,
	da bei Parameteränderungen nur die betroffenen Histogramme neu erzeugt werden müssen.
	
	Die Monte Carlo Methode ist ein Verfahren zur nummerischen Approximation von Integralen, dieses
	kommt zur Lösung der Volumengleichung in der Arbeit \cite{KPB12a} zum Einsatz. Der Fokus liegt
	der Arbeit liegt auf der physikalischen Grundlage der Gleichung. Die Arbeit verspricht eine
	Globale Beleuchtungsberechnung in Echtzeit und unterstützt unter anderen auch Mehrfachstreuungen.

	Ambiente Verdeckung kommt in der Arbeit \cite{journals/tvcg/KnissPHSM03} zum Einsatz. 
	Mit Hilfe des so genannten \textit{blurring} werden die Farbanteile des Indirekten Lichtes in einem Buffer gehalten. Dieses verfahren approximiert die Streuungseffekte, welche in Materialien auftreten können.
	Eine Erweiterung dieses Verfahrens stellt die Arbeit \cite{10.1111:j.1467-8659.2009.01464.x} dar, welche hier nur der Vollständigkeit halber erwähnt werden soll.
		
\chapter{Grundlagen}

	Der Betrachtete Algorithmus basiert auf dem Raycast-Verfahren. Grundlage ist das Modell einer Kamera,
	diese Modell besitzt einen, in einzelne Bildpunkte aufgeteilten Sensor. Für jeden Pixel wird
	ein Strahl in die Szene geschossen, für jeden Strahl wird ein Kollisionstest mit den Objekten durchgeführt.
	Gibt es einen Schnitt von Objekt und Strahl, wird die Gleichung für das dazugehörige Stahlsegment gelöst.
	Das Ergebnis der Gleichung sind Lichtintensitäten die sich auf der Sensorfläche aufsummieren.
	
	In dieser Arbeit wird die Menge der Objekte auf Kugeln reduziert. Jede Kugel repräsentiert ein mit Gas gefüllten
	Partikel. Die Dichte im Inneren jedes Partikels wird als Konstant angenommen und die Menge der Transferfunktionen
	auf Lineare beschränkt werden. Im Folgenden werden die Gleichungen für Strahlen, Kugeln und deren Schnittberechnung
	definiert. Das Kapitel wird mit der Herleitung der perspektivischen Kamera beendet.
	
	Punkte werden in dieser Arbeit wie Vektoren behandelt, sie unterscheiden sich von diesen in den Formeln dadurch das sie
	Unterstrichen sind und keinen Pfeil besitzen wie ein Vektor. $\underline p$ stellt einen Punkt und $\vec{p}$ einen
	Vektor dar. Skalarprodukte werden mit Hilfe von spitzen Klammern $\langle \vec{a}, \vec{b} \rangle$ repräsentiert.

\section{Strahl und Kugelgleichung}

	Es wird mit der Definition des Strahls begonnen, der das Zentrale Element des Algorithmus bildet. Die Formel
	beschreibt wie jeder Punkt auf dem Strahl in Abhängigkeit eines skalaren Wertes $t$, eines Stützpunktes $\underline{o}$
	und einem Richtungsvektor $r$ bestimmt werden kann. Die Gleichung definiert sich wie folgt:
	
	\begin{equation}
	  	\underline{p}(t) = \underline{o} + t \cdot \vec{r} \text{, mit } t \in \mathbb{R}
	  	\label{eq:Strahl}
	\end{equation}
	
	Jeder Punkt auf dem Strahl $\underline{p}(t)$, ergibt sich aus der Addition eines Stützpunktes $\underline{o}$ mit dem durch $t$ skalierten Richtungsvektor $\vec{r}$. Der Vektor $\vec{r}$ muss normiert sein, die Länge des Vektors muss genau 1 betragen:
	$\left\|\left|\vec{r}\right|\right| = 1$.
	Ist der Wert von $t < 0$ liegt der Punkt hinter dem Ausgangspunkt des Strahls, andernfalls
	davor oder im Fall von $t = 0$ entspricht er eben diesen.
	
	Neben dem Strahl ist die Kugel, welche die Geometrische Form der betrachteten Partikel darstellt eine wichtige Rolle und soll
	ebenfalls definiert werden, damit die Schnittpunktberechnung durchgeführt werden kann.
	
	Die Fläche einer Kugel wird im $\mathbb{R}^3$ durch den Mittelpunkt $\underline{m} = (x_0, y_0, z_0)$ und den Radius $r$ parametrisiert. Jeder Punkt auf der Kugeloberfläche lässt sich durch den Abstand zum Mittelpunkt, dem Zentrum der Kugel definieren.
	Die Folgende Formel beschreibt diese Formulierung:
	
	\begin{equation}
		(x - x_0)^2 + (y - y_0)^2 + (z - z_0)^2 = r^2
		\label{eq:KugelR3}
	\end{equation}
	
	Jeder Punkt $\underline{p} = (x, y, z)$ der die Formel \ref{eq:KugelR3} erfüllt liegt auf der Oberfläche der Kugel.
	Die Kugelgleichung \ref{eq:KugelR3} lässt sich auf beliebige $\mathbb{R}^n\text{, mit } n \in \mathbb{N}_+$ erweitern,
	und die Beschreibung ist für alle $n$ äquivalent:
	
	\begin{equation}
		\langle\underline{p} - \underline{m}, \underline{p} - \underline{m}\rangle = r^2
		\label{eq:Kugel}
	\end{equation}

	Um den Schnittpunkt zwischen Kugel und Strahl zu berechnen wird die Kugelgleichung \ref{eq:Kugel} vereinfacht.
	Der Mittelpunkt der Kugel auf den Koordinaten Ursprung verschoben, liegen alle Punkte $p$ auf der Oberfläche $O$, wenn sie denn Abstand $r$ zum Koordinatenursprung haben. Die zu erfüllende Bedingung hat demnach folgende Form:
	
	\begin{equation}
		\vert\vert\underline{p}\vert\vert = r
		\label{eq:KugelEasy}
	\end{equation}
	
	Die Gleichung \ref{eq:KugelEasy} lässt sich Quartieren und der Strahl wird anschließend in diese eingesetzt. Das Resultat
	ist eine quadratische Gleichung. Bis auf das Skalar $t$ sind alle Werte bekannt. Die Gleichung
	
	\begin{equation}
			r^2 = \langle\underline{p}, \underline{p}\rangle = \langle r(t), r(t) \rangle = \langle \underline{p} + t \cdot \vec{r}, \underline{p} + t \cdot \vec{r} \rangle
			\label{eq:StrahlInKugel}
	\end{equation}
	
	kann nach $t$ umgestellt werden und die Nullstellen berechnet werden. Es gibt entweder keine, eine oder zwei Lösungen für die
	Gleichung. Werden die bestimmten Schnittpunkte in die Gleichung des Strahls eingesetzt können die Positionen der Schnittpositionen
	berechnet werden. Die Komplette Lösungsformel definiert sich im Folgenden:
	
	\begin{equation}
		t_{1,2} = \frac{-2 \cdot \langle \underline{p}, \vec{r} \rangle \pm \sqrt{4 \cdot \langle \underline{p}, \vec{r} \rangle^2 - 4 \cdot \langle \underline{p}, \underline{p} \rangle \cdot \langle \vec{r}, \vec{r} \rangle}}{2 \cdot \langle \vec{r}, \vec{r} \rangle}.
		\label{eq:Schnittpunkte}
	\end{equation}
	

\section{Die Perspektivische Kamera}

	Um in der realen Welt Bilder aufzunehmen wird eine Kamera benötigt, hier wird eine Simulation durchgeführt
	und für diese wird das Modell einer Kamera benötigt welches sich von der \textit{camera obscura} ableitet.
	Dabei handelt es sich um das Modell einer Lochkamera, diese besteht aus einem Kasten. Auf der einen Seite
	befindet sich ein Lichtempfindliches Material, welches den Sensor bildet und auf der an gegenüberliegenden
	Seite befindet sich ein schmales Loch. Das Licht wird von der Oberfläche der Abzubildenden Objekte reflektiert
	und fällt durch Loch auf den Sensor. Bei dem Vorgang der Abbildung findet eine Vertikale und Horizontale
	Spiegelung der Szene statt. Das bedeutet das Bild der Szene wird spiegelverkehrt aufgenommen.
	
	\begin{figure}[h]
		\centering
		\def\svgwidth{5cm}
		\input{images/cameraObscura.pdf_tex}
		\caption{Schematische Darstellung des Abbildungsvorgangs der \textit{camera obscura}}
		\label{fig:cameraObscura}
	\end{figure}
	
	Die Abbildung \ref{fig:cameraObscura} zeigt den Aufbau der \textit{Camera obscura} schematisch.
	Auf der Linken Seite der Abbildung befindet sich die Bildebene, auf welche die Szene abgebildet wird.
	In der Mitte der Abbildung \ref{fig:cameraObscura} befindet sich die Wand mit dem kleinen Loch.
	Hier wird die erste Abstraktion durchgeführt, denn in der Realität kann das Loch eine gewisse Größe nicht
	unterschreiten, da andernfalls kein Licht hindurch dringen würde. Aufgrund dieser minimalen Größe kommt
	es zu einer Glättung der Abbildung. Diese Einschränkung gilt für die Computersimulation nicht, es wird
	angenommen, dass das Loch unendlich klein ist, so dass die Szene unendlich genau Abgebildet werden kann.
	
	Auf der rechten Seite in der Abbildung \ref{fig:cameraObscura} befindet sich ein Objekt in Form einer Strecke,
	welche durch die Punkte $A$ und $B$ begrenzt wird. Der Punkt $B$ befindet sich mit der Sensormitte und dem
	Loch genau auf einer Ebene und er wird auf den Punkt $B'$ abgebildet. 
	Der zweite Punkt $A$ wird auf dem Punkt $A'$ abgebildet. Genauer zeigt die Abbildung \ref{fig:cameraObscura} den
	Strahlensatz der in diesem Zusammenhang gilt. 
		 
	Die Distanz zwischen dem Sensor und dem Loch der Kamera wird mit der Variable $f$ bezeichnet. Da es sich wie
	oben bereits erwähnt um eine Computersimulation handelt kann das Modell weiter abstrahiert werden.
	In der im Computer simulierten Welt kann der Sensor auch vor dem Loch positioniert werden, was zur Folge hat,
	das die Abbildung entspiegelt wird. 
	
	Die Position der Kamera soll im Folgenden mit der Variable $\underline{p}$ bezeichnet werden. Diese ist
	mit der Position des Lochs der Lochkamera identisch und wird in der Literatur auch als Augpunkt bezeichnet.
	Für jeden Pixel der Bildebene wird mindestens ein Strahl erzeugt, welcher als Stützpunkt die Position
	der Kamera erhält und der Richtungsvektor ist der Normierte Vektor von der Kamerapostion zum Pixel.
	Zur Vereinfachung wird angenommen, dass die Bildebene parallel zur xy-Ebene des Koordinatensystems der Szenen ist
	und der Augpunkt genau im Ursprung von diesem liegt.
	Die Fläche des Sensors wird in $W \cdot H$ Pixeln unterteilt.
	Für jeden Pixel $(x, y)$ wird ein Strahl $s$ in die Szene geschossen, dessen Stützpunkt der Augpunkt der Kamera
	ist und der Richtungsvektor der normierte Vektor $\vec{r}(x, y)$ welche sich ohne wie folgt definiert.
	
	\begin{equation}
		\vec{r}(x, y) = \left( 
							\begin{array}{l}
								(2 \frac{x}{W} - 1) \cdot \frac{W}{H} \\
								-2 \frac{y}{H} + 1 \\
								f
							\end{array}							
					 	\right)
		\label{eq:direction}
	\end{equation}
	
	Es ist gilt zu beachten das der Richtungsvektor noch normiert werden muss: $\vec{r}(x, y) =  \frac{\vec{r}(x, y)}{\left|\left|\vec{r}(x, y)\right|\right|}$. 
	Jeder Kamera Strahl, im folgenden auch als Primärstrahl bezeichnet hat die folgende Form:
	
	\begin{equation}
		\underline{s} (x, y) = \underline{p} + t \cdot \vec{r}(x, y)
		\label{eq:primaryRay}
	\end{equation}
	
	Die Kamera kann an dieser Stelle nur entlang der z-Achse Blicken, eine Rotation im Raum lässt sich durch die Multiplikation
	einer Rotationsmatrix mit den Richtungsvektoren bewerkstelligen. Die Position des Kameramodells ist frei Wählbar, es genügt den
	Augpunkt zu verschieben. Mit Hilfe der Distanz $f$, der fokalen Länge kann ein Zoomeffekt der Kamera erzielt werden.
	
\chapter{Beleuchtungsmodell}

	Der Ausgangspunkt dieses Kapitels ist das Emission-Absorption-Modell von Nelson Max. In diesem werden die Wechselwirkung des Lichtes auf die Absorption und die Emission beschränkt, die Streuung des Lichtes wird in
	dieser Arbeit vernachlässigt.
	Die Gleichung der Beleuchtungsberechnung unterteilt sich in zwei Summanden:

	\begin{equation}
		I = I_A + I_E
		\label{eq:EAModell}
 	\end{equation}
	
	Die Variable $I$ repräsentiert die von dem Sensor aufsummierte Licht Intensität, in der Gleichung \ref{eq:EAModell}. Der Term $I_A$ beschreibt die Abschwächung der Hintergrundbeleuchtung durch das mit Gas gefüllte Volumen
	und $I_E$ die Emission.
	Die Abschwächung entspricht dem Produkt des Hintergrundlichtes mit einem Wert für die Transparenz, dieser ist Abhängigkeit von der Streckenlänge $D \in \mathbb{R}_+$ die das Licht durch das Volumen zurücklegen muss. 
	Dieser Zusammenhang wird in der folgenden Gleichung dargestellt:
	
	\begin{equation}
		I_A = I_B \cdot T(D) 
		\label{eq:AModell}
	\end{equation}
	
	Die Transparenz wird dabei durch die Funktion $T(D)$ ermittelt:
	
	\begin{equation}
		T(s) = exp(- \int\limits_{0}^{s} \tau(t) dt)
		\label{eq:Transparenz}
	\end{equation}
	
	Der Grad der Abschwächung wird mit $\tau(t) = \sigma(\nu(t))$ bestimmt. Die Funktion $\tau(t)$ wird in dieser Funktion als Transferfunktion bezeichnet, die die Dichte des Volumens auf einen Wert für die Absorption abbildet.
	Die Menge dieser Funktion wird in dieser Arbeit auf lineare Funktionen beschränkt. 
	$\sigma(\nu(t)) = \lambda \nu$ skaliert mit einem konstanten Faktor $\lambda \in (0,1]$ den Wert für die Dichte des Volumens.
	Diese wird mit der Funktion $\nu(t) = \kappa$ auf einen konstanten Faktor $\kappa$ abgebildet. Die Funktion $T$ aus der Gleichung \ref{eq:Transparenz} berechnet diese mit Hilfe einer Länge $s \in \mathbb{R}_+$, um die Formeln
	zu Vereinfachen wird eine weitere Funktion definiert, welche die Länge aus der Differenz einer hinteren und einer 
	vorderen Position berechnet:
	
	\begin{equation}
		T'(t_n, t_f) = exp(- \int\limits_{t_n}^{t_f} (\lambda\kappa) dt)
		\label{eq:ATransparenz}
	\end{equation}
	
	In der Funktion \ref{eq:ATransparenz} steht die Variable $t_n$ den Eintrittspunkt in das Volumen und $t_f$ für den Austrittspunkt.
	Das Produkt von $\lambda$ und $\kappa$ ist ein konstanter Wert aus diesem Grund lässt er sich aus dem Integral herausziehen und es existiert eine analytisch Lösung:
	
	\begin{equation}
		T'(t_n, t_f) = exp(-\lambda\kappa \cdot \int\limits_{t_n}^{t_f} dt) = e^{-\lambda\kappa \cdot (t_f - t_n)}
		\label{eq:ATransparenzSol}
	\end{equation}
	
	Die Emission wird mit der Variable $I_E$ bezeichnet und wird zunächst sehr allgemein definiert:	
	
	\begin{equation}
		I_E = \int\limits_{0}^{D} g(s) \cdot T'(s, D) ds
		\label{eq:EModell}
	\end{equation}
	
	Die Funktion $g(s)$ wird als Quellterm bezeichnet, dieser stellt eine beliebige Funktion, 
	in Abhängigkeit einer Position $s$ dar. Das bei $s$ emittierte Licht wird auch durch das das Volumen abgeschwächt.
	Wird die Formeln für die Absorption und die Emission in die Gleichung \ref{eq:EAModell} eingesetzt, dann hat die 
	Formel für die komplette Beleuchtungsberechnung die folgende Form: 
	
	\begin{equation}
		I = I_B \cdot T(D) + \int\limits_{0}^{D} g(s) \cdot T'(s, D) ds
		\label{eq:CompEAModell}
	\end{equation}
	
	Die Gleichung \ref{eq:CompEAModell} ist noch sehr allgemein. Es wird beschrieben wie sich das Hintergrundlicht durch
	das Volumen abschwächt und unter der Beschränkung auf lineare Transferfunktion existiert eine analytische Lösung. 
	Das Integral über den Quellterm wurde bisher noch nicht genauer definiert. In den nächsten beiden Absätzen
	werden zwei verschiedene Funktionen für $g(s)$ definiert, beide werden für den eigentlichen Algorithmus benötigt.
	
\section{Einfaches Dichte Modell}
	
	Ein primärer Strahl, auf als Sichtstrahl bezeichnet wird vom Sensor aus in die Szene geschossen.
	Die folgende Abbildung stellt diesen Vorgang schematisch dar. 
	
	\begin{figure}[h]
		\centering
		\def\svgwidth{10cm}
		\input{images/standartAbsorption.pdf_tex}
		\caption{Die Abbildung zeigt einen primären Sichtstrahl, der ein Partikel schneidet. Auf der rechten Seite der Abbildung symbolisiert eine kleine Sonne die Hintergrundbeleuchtung und auf der linken ein Auge den Sensor.}
		\label{fig:eaSchematisch}
	\end{figure}
	
	Der Sichtstrahl schneidet ein Partikel an den Positionen $t_n$ und $t_f$.
	Es wird eine Hintergrundbeleuchtung $I_B$ mit einer konstanten Intensität definiert.
	Diese wird entlang des Strahls durch das Gas im Volumen mit der bereits ermittelten Formel
	$I_A = I_B \cdot exp(-\lambda\kappa(t_f - t_n))$ abgeschwächt. 
	Die Transferfunktion für die Emission wird durch die Funktion $c(\nu(t)) = \lambda I_c \cdot \nu(t)$ beschrieben, wobei $I_c$ als konstante Lichtintensität gewählt wird und $\nu(t) = \kappa$ die Dichte auf den konstanten Wert
	$\kappa$ abbildet. Zunächst wird die Emissionsgleichung mit den Eintritts und Austrittspunkten parametrisiert:
	
	\begin{equation}
		I_E = \int\limits_{t_n}^{t_f} T'(t, t_f) \cdot g(t) dt
		\label{eq:EModellEDM}
	\end{equation}
	
	Die Quellfunktion wird in diesem Modell definiert als $g(t) = \lambda\kappa I_c$, so das die Formel für
	die Emission ausgeschrieben der folgenden Form entspricht:
	
	\begin{equation}
		I_E = \int\limits_{t_n}^{t_f} exp(-\lambda\kappa (t_f - t)) \cdot \lambda\kappa I_c dt
		\label{eq:EModellEDMA}
	\end{equation}
	
	Für die Gleichung \ref{eq:EModellEDMA} lässt sich mit Hilfe der Substitutionsregel eine analytische Lösung finden:
	
	\begin{align}
		I_E &= I_C \cdot (1 - e^{-\lambda\kappa (t_f - t_n)}) = I_C \cdot (1 - T'(t_n, t_f)) \notag\\
		I_E &= I_C \cdot \Theta(t_n, t_f)
	\end{align} 
	
	Die gesamte Gleichung zur Beleuchtung eines Partikels entspricht der folgenden Form: 
	
	\begin{equation}
		I = I_B \cdot T'(t_n, t_f) + I_C \cdot \Theta(t_n, t_f)
	\end{equation}
	
\section{Erweitertes Dichte Modell}

	Kern dieser Arbeit ist eine Erweiterung des eben vorgestellten Beleuchtungsmodells. Neben der Hintergrundbeleuchtung
	wird die Beleuchtungsberechnung für eine beliebige Anzahl von Punkt und Richtungslichtquellen erweitert. Zur
	Herleitung der Gleichung werden die Lichtquellen auf die Hintergrundbeleuchtung und eine Punktlichtquelle beschränkt.
	Anschließend wird die gefundene Lösung wieder für beliebig viele Lichtquellen verallgemeinert.
	Die folgende Abbildung soll zunächst die Vereinfachung veranschaulichen.

	\begin{figure}[h]
		\centering
		\def\svgwidth{10cm}
		\input{images/exStandartAbsorption.pdf_tex}
		\caption{Die Notation ist die selbe wie in der Abbildung \ref{fig:eaSchematisch}. Diese Abbildung ergänzt das Modell um eine Punktlichtquelle und es werden die sekundäre Strahlen zu dieser dargestellt.}
		\label{fig:exEaSchematisch}
	\end{figure}
	
	Die Grundlage für dieses erweiterte Modell die selbe Gleichung \ref{eq:CompEAModell} wie für die vorherige.
	Diese wird zur besseren Übersicht an dieser Stelle mit den Notationen aus dem vorhergehenden Abschnitt wiederholt:
	
	\begin{equation}
		I = I_B \cdot T'(t_n, t_f) + \int\limits_{t_n}^{t_f} g(t) \cdot T'(t, t_f) dt
		\label{eq:CompEAModellN}
	\end{equation}
	
	Der erste Teil der Gleichung  beschreibt die Absorption und bleibt unverändert. Die Modelle unterscheiden sich nur durch die Quellfunktion $g(t)$. 
	Diese berechnet die Kugelfarbe an jedem Punkt $t \in [t_n, t_f]$ entlang des Sichtstrahls und wird anschließend durch das Volumen abgeschwächt. 
	Einfluss auf die Farbe der Kugeln hat die Intensität der Punktlichtquelle und die Emission des Gases. 
	Die Funktion $l(t)$ liefert die Länge Schnittlänge des Sekundärstrahls mit dem Partikel. 
	Ein Sekundärstrahl ist in diesem Zusammenhang ein Strahl, welcher ausgehend von der Position $t$ in die Richtung der Punktlichtquelle geschossen wird. 
	Die Farbe an der Position $t$ bestimmt sich mit der Hilfe der Länge $l(t)$ und dem Modell aus dem vorherigen Abschnitt durch die folgende Rechenvorschrift:
	
	\begin{equation}
		g(t) = I_L \cdot T'(0, l(t)) + I_c \cdot \Theta(0, l(t))
		\label{eq:quellfunktion}
	\end{equation}
	
	Die Variable $I_L$ entspricht der Intensität der Punktlichtquelle.
	Punkt und Richtungslichtquellen unterscheiden sich durch die Berechnung des Richtungsvektors für die Sekundärstrahlen.
	Der Richtungsvektor der Sekundärstrahlen für Punktlichtquellen bestimmt sich aus der normierten Differenz zwischen der Position der Lichtquelle und der Position auf dem Sichtstrahl in Abhängigkeit von $t$. 
	Richtungslichtquellen werden bereits mit einem normierten Richtungsvektor parametrisiert. Die Invertierung von diesem entspricht dem Richtungsvektor des Sekundärstrahls. Wird die Gleichung \ref{eq:quellfunktion} in die Gleichung \ref{eq:CompEAModellN} ergibt sich daraus ein Integral für das keine analytische Lösung existiert:
	
	\begin{equation}
		I = I_B \cdot T'(t_n, t_f) + \int\limits_{t_n}^{t_f} ( I_L \cdot T'(0, l(t)) + I_c \cdot \Theta(0, l(t)) ) \cdot T'(t, t_f) dt
		\label{eq:CompEAModell1}
	\end{equation}

\chapter{Implementation}

\chapter{Evaluation und Diskussion}
\section{Fazit}

%\chapter{title}
%
%\chapter{ein kapitel}
%\section{eine Grafik}
%\begin{figure}[htbp]
%	\centering
%		\includegraphics{test.png}
%	\caption{beschriftung}
%	\label{fig:diplominf}
%\end{figure}

\end{document}