\documentclass[hyperref,german,beleg,final,lof]{cgvpub}
%weitere Optionen zum Ergänzen (in eckigen Klammern):
% 
% female	weibliche Titelbezeichnung bei Diplom
% bibnum	numerische Literaturschlüssel
% final 	für Abgabe	
% lof			Abbildungsverzeichis
% lot			Tabellenverzeichnis
% noproblem	keine Aufgabenstellung
% notoc			kein Inhaltsverzeichnis
% twoside		zweiseitig

\usepackage{tabularx}
\usepackage{listings}
\usepackage{subfigure} 

\lstset{
language=C,
basicstyle=\small\sffamily,
numbers=left,
numberstyle=\tiny,
frame=tb,
columns=fullflexible,
showstringspaces=false
}

\renewcommand{\lstlistingname}{Pseudocode}

\author{Josef Schulz}
\title{Ground-Truth-Renderer für Partikelbasierte Daten}
\birthday{20. Oktober 1989}
\placeofbirth{Dresden}
\matno{3658867}

\betreuer{Dipl-MedienInf. Joachim Staib}
\bibfiles{literatur.bib}
\problem{
Die Darstellung von Partikeldaten mittels Kugelglyphen ist in wissenschaftlichen Visualisierung
inzwischen etabliert. Gerade bei dichten Datensätzen stellen kompakte Anordnungen von sehr vielen
Kugeln jedoch ein Problem für die Erkennbarkeit der zu visualisierenden Vorgänge dar. Eine Möglichkeit, diesem Problem zu begegnen ist es, über Blinn-Phong-Beleuchtung hinausgehende Effekte wie
globale Schatten oder den Einsatz von Methoden aus dem Volume-Rendering zu integrieren. Durch
deren Komplexität muss in Echtzeitvisualisierungen jedoch auf teilweise grobe Approximationen zu-
rückgegriffen werden. Die Einschätzung der Approximationsqualität fällt häufig schwer, da keine Visualisierung des exakten Verfahrens verfügbar ist.
Ziel dieser Belegarbeit ist die Umsetzung eines CPU-Renderers für Partikeldaten, der eine Reihe
von erweiterten Visualisierungseffekten unterstützt. Er soll die Grundlage für Ground-Truth-
Visualisierungen bieten.
Zunächst soll eine geeignete Softwarearchitektur konzipiert und umgesetzt werden. Die Partikel sollen als mit lichtemittierendem und ?absorbierendem Gas gefüllte Kugeln interpretiert werden. Es sollen anschließend Methoden entwickelt werden, um einen physikalisch plausiblen globalen Schattenwurf und Lichttransport für eine beliebige Anzahl an Punkt- und Richtungslichtquellen zu ermöglichen.
Die dafür notwendigen Gleichungen für Kugeln mit konstanter Dichte und Emission, sowie linearer
Absorption, sollen soweit wie möglich analytisch bestimmt und, sobald nicht mehr möglich, mittels
möglichst exakter numerischer Integratoren ausgewertet werden.

Die Teilaufgaben umfassen:

\begin{itemize}
\item Umfassende Literaturrecherche zur globalen Beleuchtungsrechnung in der Volumen Visualisierung
\item Schrittweise Konzeption und Umsetzung einer erweiterbaren Architektur zum Erzeugen von Ground-Truth-Bildern:

	\begin{enumerate}
		\item Zunächst als Raytracer für opake Kugeln, der globale Schatteneffekte von frei
		positionierbaren Punkt- und Richtungslichtquellen unterstützt
		\item Umsetzung eines Renderers, der Kugeln als Volumen nach dem Emissions-Absorptions-Modell rendert, dabei analytische Bestimmung des Volume-Rendering-Integrals, einschließlich Integration direkter Beleuchtung unverdeckter Lichtquellen
		\item Erweiterung zu verdeckten Lichtquellen und Bestimmung der Lichtstärke- und Farbe
		für Lichtstrahlen durch verdeckende Kugeln
	\end{enumerate}
\item Unterstützung für ein Standardformat wie VRML
\item Evaluation in Bezug auf Korrektheit, Bildartefakte und (numerische) Grenzfälle
\end{itemize}
\newpage
Optional:
\begin{itemize}
\item Unterstützung für Refraktionseffekte
\item Unterstützung komplexerer Materialtypen
\end{itemize}
}

\copyrighterklaerung{Hier soll jeder Autor die von ihm eingeholten
Zustimmungen der Copyright-Besitzer angeben bzw. die in Web Press
Rooms angegebenen generellen Konditionen seiner Text- und
Bild"ubernahmen zitieren.}

\acknowledgments{Die Danksagung...}
	
	 
\begin{document}

\nocite{*}

\chapter{Einleitung}

\section{Motivation}
	
	Bei einer Vielzahl von Messvorgängen oder Simulationen ergeben sich partikelbasierte Datensätze.
	Erst durch eine geeignete Darstellung lassen sich diese optimal auswerten und analysieren.
	Das Ziel von Visualisierungen ist die Unterstützung der menschlichen Wahrnehmung bei der Auswertung
	der erhobenen Daten.
	Dabei geht es nicht um eine bloße optische Verschönerung der Darstellung, sondern darum bestimmte Eigenschaften 
	in den Fokus des Betrachters zu rücken und Tendenzen hervorzuheben.
	Die Darstellung von dreidimensionalen Partikeln auf einer zweidimensionalen Ebene ist mit Problemen verbunden.
	Diese sollen im Folgenden näher betrachtet werden.
	
	Die Fähigkeit des Menschen die Tiefe von Objekten in einer Szene schätzen zu können wird durch ein System ermöglicht,
	das den Wert für die Tiefe auf unterschiedlichen Wegen bestimmt. Ein Weg besteht in der Schätzung der Entfernung über
	die Epipolargeometrie. Bei der Betrachtung eines zweidimensionalen Bildes kann diese Variante lediglich die Bildschirmebene erfassen, jedoch nicht die Tiefe der abgebildeten Objekte. Die Objekttiefen in der Abbildung
	kann der Mensch anhand von Überdeckungen und durch eine perspektivische Transformation der Objekte abschätzen.
	Damit Überdeckungen erkannt und die abgebildeten Objekte von einander unterschieden werden können, müssen sich diese
	sich durch einen Kontrast voneinander abheben. Schattierungen lösen dieses Problem. 
	Zudem wirkt ein Kugelgylph im Vergleich zur Darstellung durch einen einfarbigen Kreis plastischer. 	
	Überdeckungen verhindern allerdings, dass der Betrachter die dahinterliegenden Partikel sehen kann. 
	Ganze Strukturformen, zu denen beispielsweise Höhlen zählen, können deshalb nicht erfasst werden.
	Eine Möglichkeit dieses Problem zu lösen besteht darin die Partikel transparent abzubilden, damit trotz
	der Überdeckung alle Strukturen sichtbar bleiben.
	Die zuvor erwähnten Schattierungen ergeben sich in der realen Welt aus der Wechselwirkung zwischen Licht und
	Materie. 
	Eine weitere vom Menschen, zur Bestimmung von Objekttiefen genutzte Möglichkeit, ist die der Rekonstruktion von Lichtwegen.
	
	Lokale Beleuchtungsmodelle eignen sich zur Berechnung von Schattierung, unterstützen die Wahrnehmung der Tiefe jedoch nur bedingt.
	Es existiert eine Reihe von Beleuchtungsmodellen die die Szene global beleuchten. 
	Der Einsatz solcher Modelle stellt eine bessere Unterstützung der Wahrnehmung dar.
	Existierenden Algorithmen müssen auf Grund einer hohe Rechenkomplexität der globalen Beleuchtung an vielen stellen approximativ vereinfachen.
	Insbesondere GPU-Implementierungen sind auf diese angewiesen um die globale Beleuchtung in Echtzeit zur realisieren.
	
	Ziel dieser Arbeit ist die Entwicklung eines CPU-Renderes für Partikel, bei dem der Fokus auf der Genauigkeit und nicht auf der Geschwindigkeit der Berechnung liegt.
	Die Partikel werden als mit Gas gefüllte Kugeln interpretiert und mit einem Verfahren der direkten Volumen-Darstellung
	gezeichnet. Direkte Verfahren zerlegen die Daten nicht in einem Vorverarbeitungsschritt in Netze aus Polygonen,
	sondern visualisieren die Daten in einem Schritt.
	Die Grundlage des Verfahrens stellt ein Volumenintegral dar, dessen Lösung mit Hilfe eines Raycast-Algorithmus
	dargestellt wird. Dabei handelt es sich um ein halb-analytisches Verfahren.
	Vorteile analytischer Lösungen liegen in der Genauigkeit und der Geschwindigkeit der Berechnung.  
	Die erzeugten Bilder dienen als \textit{Ground-Truth} Information zur Evaluation von approximativen Implementierungen. 
	
\section{Struktur der Arbeit}
	
	Im Folgenden werden verwandte Arbeiten vorgestellt, gefolgt von den Grundlagen des Raycast-Verfahrens, das nach
	der Einführung von Strahl und Kugelgleichungen mit der Herleitung der Perspektivischen Kamera schließt.
	Das vierte Kapitel stellt das verwendete Beleuchtungsmodell vor. Details zur Implementierung sind im fünften Kapitel
	zu finden. Beendet wird die Arbeit mit einer Auswertung des Verfahren in Abhängigkeit gewählter Parameter.
	
\chapter{Verwandte Arbeiten}

	Die Arbeit \cite{JSYR14} fast verschiedenen Algorithmen zur Beleuchtung und interaktiven Darstellung
	von Volumendaten in Echtzeit zusammen.
	Vorgestellte Algorithmen werden hinsichtlich ihrer technischen Umsetzung, Performance und ihrer
	Unterstützung für die menschliche Wahrnehmung klassifiziert.  
	Diese Kriterien erleichtern die Auswahl des Verfahrens für den Entwickler in Abhängigkeit der Anwendung.
	Bei den Verfahren handelt es sich um approximative Varianten mit dem Ziel interaktive Bildraten zu
	gewährleisten.
	
	Die Grundlagen des Emission-Absorption-Modells werden von Nelson Max in der Arbeit \cite{Max:1995:OMD:614258.614298} beschrieben.
	Das Volumenintegral des Emission-Absorption-Modells wird hergeleitet und Stück für Stück mit Effekten
	angereichert. Neben der einfachen Streuung und der Schattenberechnung wird auch die Mehrfachstreuung vorgestellt.
	Beendet wir die Arbeit mit der Vorstellung anwendbarer Lösungsverfahren.
	
	In der Publikation \cite{conf/pg/JungPP98} wird ein zum Teil analytisches Verfahren vorgestellt. 
	Der Algorithmus wird zur Darstellung von Voxelgittern eingesetzt.  
	Hierbei wird jedem Voxel wird eine Dichte zu geordnet, aus welcher ein Wert für die Transparenz abgeleitet wird.
	Hinter dem Voxelgitter befindet sich eine Lichtquelle, deren Lichtintensität durch das Volumen unterschiedlich stark abgeschwächt wird. 
	Die Integrale der Gleichungen werden streckenweise analytisch gelöst, wodurch ein Zuwachs an 
	Genauigkeit erreicht wird. 
	
	In der Arbeit \cite{journals/tvcg/AmentSW13} geht es um einen alternativen Ansatz, welcher nicht auf dem Emission- und Absorptionsmodell aufbaut, 
	sondern von einem Punkt aus Stahlen in die umliegende Nachbarschaft verfolgt um den
	Grad an Verdeckung an diesem Punkt zu bestimmen. Je mehr Segmente mit hoher Dichte in der Nachbarschaft liegen,
	umso geringer ist der Anteil des Lichtes, welches eben jenen Punkt erreicht. Ein Geschwindigkeitszuwachs wird durch eine im Vorfeld integrierte Transferfunktion erreicht. 
	Ein Vorteil dieses Verfahrens stellt die Güte der erzeugten Schatten da.

	Photonen-Mapping ist ein Verfahren zur Erzeugung von globalen Beleuchtungseffekten.
	Dieses kommt in der Arbeit \cite{JKRY12} zur Beleuchtung von Volumendaten in Echtzeit zum Einsatz.
	Die dazu benötigte Photonenmap wir dabei mit Histogrammen realisiert. 
	Mit deren Hilfe können Änderungen in interaktiven Bildwiederholungsraten ermöglicht werden,
	da bei Parameteränderungen nur die betroffenen Histogramme neu erzeugt werden müssen.
	
	Die Monte Carlo-Methode ist ein Verfahren zur numerischen Approximation von Integralen.
	Diese kommt zur Lösung der Volumengleichung in der Arbeit \cite{KPB12a} zum Einsatz. 
	Der Fokus der Arbeit liegt auf der physikalischen Grundlage dieses Verfahrens. Die Arbeit verspricht eine
	globale Beleuchtungsberechnung in Echtzeit und unterstützt unter anderem auch Mehrfachstreuungen.

	Ambiente Verdeckung kommt in der Arbeit \cite{journals/tvcg/KnissPHSM03} zum Einsatz. 
	Mit Hilfe des sogenannten \textit{blurring} werden die Farbanteile des indirekten Lichtes in einem Buffer gehalten. 
	Dieses Verfahren approximiert die Streuungseffekte, welche in Materialien auftreten können.
	Eine Erweiterung dieses Verfahrens stellt die Arbeit \cite{10.1111:j.1467-8659.2009.01464.x} dar, welche hier nur der Vollständigkeit halber erwähnt werden soll.
		
\chapter{Grundlagen}

	Der verwendete Algorithmus basiert auf dem Raycast-Verfahren. Grundlage ist das Modell einer Kamera.
	Diese Modell besitzt einen, in einzelne Bildpunkte aufgeteilten Sensor. Für jeden Pixel wird
	ein Strahl in die Szene geschossen und für jeden Strahl wird ein Kollisionstest mit den Objekten durchgeführt.
	Gibt es einen Schnitt von Objekt und Strahl, wird die Gleichung für das dazugehörige Stahlsegment gelöst.
	Das Ergebnis der Gleichung sind Lichtintensitäten die sich auf der Sensorfläche aufsummieren.
	
	In dieser Arbeit wird die Menge der Objekte auf Kugeln reduziert. Jede Kugel repräsentiert einen mit Gas gefüllten
	Partikel. Die Dichte im Inneren jedes Partikels wird als konstant angenommen und die Menge der Transferfunktionen
	auf Lineare beschränkt. Im Folgenden werden die Gleichungen für Strahlen, Kugeln und deren Schnittberechnung
	definiert. Das Kapitel wird mit der Herleitung der perspektivischen Kamera beendet.
	
	Punkte werden in dieser Arbeit wie Vektoren behandelt. Sie unterscheiden sich von diesen in den Formeln dadurch, dass sie
	unterstrichen sind und nicht mit einem Pfeil gekennzeichnet sind: $\underline p$ stellt einen Punkt und $\vec{p}$ einen
	Vektor dar. Skalarprodukte werden mit Hilfe von spitzen Klammern $\langle \vec{a}, \vec{b} \rangle$ repräsentiert.

\section{Strahl und Kugelgleichung}

	Es wird mit der Definition des Strahls begonnen, der das zentrale Element des Algorithmus bildet. Die Formel
	beschreibt wie jeder Punkt auf dem Strahl in Abhängigkeit eines skalaren Wertes $t$, eines Stützpunktes $\underline{o}$
	und einem Richtungsvektor $r$ bestimmt werden kann. Die Gleichung definiert sich wie folgt:
	
	\begin{equation}
	  	\underline{p}(t) = \underline{o} + t \cdot \vec{r} \text{, mit } t \in \mathbb{R}
	  	\label{eq:Strahl}
	\end{equation}
	
	Jeder Punkt auf dem Strahl $\underline{p}(t)$, ergibt sich aus der Addition eines Stützpunktes $\underline{o}$ mit dem durch $t$ skalierten Richtungsvektor $\vec{r}$. Der Vektor $\vec{r}$ muss normiert sein, die Länge des Vektors muss genau 1 betragen:
	$\left\|\left|\vec{r}\right|\right| = 1$.
	Ist der Wert von $t < 0$ liegt der Punkt hinter dem Ausgangspunkt des Strahls, andernfalls
	davor oder im Fall von $t = 0$ entspricht er eben diesem.
	
	Neben dem Strahl spielt die Kugel, welche die geometrische Form der betrachteten Partikel darstellt, eine wichtige Rolle und soll
	ebenfalls definiert werden, damit die Schnittpunktberechnung durchgeführt werden kann.
	
	Die Fläche einer Kugel wird im $\mathbb{R}^3$ durch den Mittelpunkt $\underline{m} = (x_0, y_0, z_0)$ und den Radius $r$ parametrisiert. Jeder Punkt auf der Kugeloberfläche lässt sich durch den Abstand zum Mittelpunkt, dem Zentrum der Kugel, definieren.
	Die folgende Formel beschreibt diese Formulierung:
	
	\begin{equation}
		(x - x_0)^2 + (y - y_0)^2 + (z - z_0)^2 = r^2
		\label{eq:KugelR3}
	\end{equation}
	
	Jeder Punkt $\underline{p} = (x, y, z)$ der die Formel \ref{eq:KugelR3} erfüllt, liegt auf der Oberfläche der Kugel.
	Die Kugelgleichung \ref{eq:KugelR3} lässt sich auf beliebige $\mathbb{R}^n\text{, mit } n \in \mathbb{N}_+$ erweitern.
	Die Beschreibung ist für alle $n$ äquivalent:
	
	\begin{equation}
		\langle\underline{p} - \underline{m}, \underline{p} - \underline{m}\rangle = r^2
		\label{eq:Kugel}
	\end{equation}

	Um den Schnittpunkt zwischen Kugel und Strahl zu berechnen, wird die Kugelgleichung \ref{eq:Kugel} vereinfacht.
	Wird der Mittelpunkt der Kugel auf den Koordinatenursprung verschoben, liegen alle Punkte $p$ auf der Oberfläche $O$, wenn sie denn Abstand $r$ zum Koordinatenursprung haben. Die zu erfüllende Bedingung hat demnach folgende Form:
	
	\begin{equation}
		\vert\vert\underline{p}\vert\vert = r
		\label{eq:KugelEasy}
	\end{equation}
	
	Nach Quadrieren der Gleichung \ref{eq:KugelEasy} wird der Strahl anschließend in diese eingesetzt. Das Resultat
	ist eine quadratische Gleichung. Bis auf das Skalar $t$ sind alle Werte bekannt. Die Gleichung
	
	\begin{equation}
			r^2 = \langle\underline{p}, \underline{p}\rangle = \langle r(t), r(t) \rangle = \langle \underline{p} + t \cdot \vec{r}, \underline{p} + t \cdot \vec{r} \rangle
			\label{eq:StrahlInKugel}
	\end{equation}
	
	kann nach $t$ umgestellt und die Nullstellen berechnet werden. Es gibt entweder keine, eine oder zwei Lösungen für die
	Gleichung. Werden die bestimmten Schnittpunkte in die Gleichung des Strahls eingesetzt, können die Positionen der Schnittpositionen
	berechnet werden. Die komplette Lösungsformel definiert sich wie folgt:
	
	\begin{equation}
		t_{1,2} = \frac{-2 \cdot \langle \underline{p}, \vec{r} \rangle \pm \sqrt{4 \cdot \langle \underline{p}, \vec{r} \rangle^2 - 4 \cdot \langle \underline{p}, \underline{p} \rangle \cdot \langle \vec{r}, \vec{r} \rangle}}{2 \cdot \langle \vec{r}, \vec{r} \rangle}.
		\label{eq:Schnittpunkte}
	\end{equation}
	

\section{Die perspektivische Kamera}

	Um in der realen Welt Bilder aufzunehmen wird eine Kamera benötigt. Hier wird eine Simulation durchgeführt
	und für diese wird das Modell einer Kamera benötigt welches sich von der \textit{camera obscura} ableitet.
	Dabei handelt es sich um das Modell einer Lochkamera, diese besteht aus einem Kasten. Auf der einen Seite
	befindet sich ein lichtempfindliches Material, welches den Sensor bildet und auf der an gegenüberliegenden
	Seite befindet sich ein schmales Loch. Das Licht wird von der Oberfläche der abzubildenden Objekte reflektiert
	und fällt durch das Loch auf den Sensor. Bei dem Vorgang der Abbildung findet eine vertikale und horizontale
	Spiegelung der Szene statt. Das führt dazu, dass das Bild der Szene wird spiegelverkehrt aufgenommen wird.
	
	\begin{figure}[h]
		\centering
		\def\svgwidth{5cm}
		\input{images/cameraObscura.pdf_tex}
		\caption{Schematische Darstellung des Abbildungsvorgangs der \textit{camera obscura}}
		\label{fig:cameraObscura}
	\end{figure}
	
	Die Abbildung \ref{fig:cameraObscura} zeigt den Aufbau der \textit{Camera obscura} schematisch.
	Auf der Linken Seite der Abbildung befindet sich die Bildebene, auf welche die Szene abgebildet wird.
	In der Mitte der Abbildung \ref{fig:cameraObscura} befindet sich die Wand mit dem kleinen Loch.
	Hier wird die erste Abstraktion durchgeführt, denn in der Realität kann das Loch eine gewisse Größe nicht
	unterschreiten, da andernfalls kein Licht mehr durch das Loch dringen würde. Auf Grund dieser minimalen Größe kommt
	es zu einer Glättung der Abbildung. Diese Einschränkung gilt für die Computersimulation nicht. 
	Es wird angenommen, dass das Loch unendlich klein ist und deshalb die Szene unendlich genau abgebildet werden kann.
	
	Auf der rechten Seite in der Abbildung \ref{fig:cameraObscura} befindet sich ein Objekt in Form einer Strecke,
	welche durch die Punkte $A$ und $B$ begrenzt wird. Der Punkt $B$ befindet sich mit der Sensormitte und dem
	Loch genau auf einer Ebene und wird auf den Punkt $B'$ abgebildet. 
	Der zweite Punkt $A$ wird auf dem Punkt $A'$ abgebildet. Genauer zeigt die Abbildung \ref{fig:cameraObscura} den
	Strahlensatz der in diesem Zusammenhang gilt. 
		 
	Die Distanz zwischen dem Sensor und dem Loch der Kamera wird mit der Variable $f$ bezeichnet. Da es sich wie
	oben bereits erwähnt um eine Computersimulation handelt kann das Modell weiter abstrahiert werden.
	In der im Computer simulierten Welt kann der Sensor auch vor dem Loch positioniert werden, was zur Folge hat,
	dass die Abbildung entspiegelt wird. 
	
	Die Position der Kamera soll im Folgenden mit der Variable $\underline{p}$ bezeichnet werden. Diese ist
	mit der Position des Lochs der Lochkamera identisch und wird in der Literatur auch als Augpunkt bezeichnet.
	Für jeden Pixel der Bildebene wird mindestens ein Strahl erzeugt, welcher als Stützpunkt die Position
	der Kamera erhält. Der Richtungsvektor ist der normierte Vektor von der Kamerapostion zum Pixel.
	Zur Vereinfachung wird angenommen, dass die Bildebene parallel zur xy-Ebene des Koordinatensystems der Szenen ist
	und der Augpunkt genau im Ursprung von diesem liegt.
	Die Fläche des Sensors wird in $W \times H$ Pixeln unterteilt.
	Für jeden Pixel $(x, y)$ wird ein Strahl $s$ in die Szene geschossen, dessen Stützpunkt der Augpunkt der Kamera
	ist. Der Richtungsvektor ist der normierte Vektor $\vec{r}(x, y)$, welcher sich wie folgt definiert.
	
	\begin{equation}
		\vec{r}(x, y) = \left( 
							\begin{array}{l}
								(2 \frac{x}{W} - 1) \cdot \frac{W}{H} \\
								-2 \frac{y}{H} + 1 \\
								f
							\end{array}							
					 	\right)
		\label{eq:direction}
	\end{equation}
	
	Es ist zu beachten, dass der Richtungsvektor noch normiert werden muss: $\vec{r}(x, y) =  \frac{\vec{r}(x, y)}{\left|\left|\vec{r}(x, y)\right|\right|}$. 
	Jeder Kamerastrahl, im folgenden auch als Primärstrahl bezeichnet, hat die folgende Form:
	
	\begin{equation}
		\underline{s} (x, y) = \underline{p} + t \cdot \vec{r}(x, y)
		\label{eq:primaryRay}
	\end{equation}
	
	Die Kamera kann an dieser Stelle nur entlang der z-Achse blicken. Eine Rotation im Raum lässt sich durch die Multiplikation
	einer Rotationsmatrix mit den Richtungsvektoren bewerkstelligen. Die Position des Kameramodells ist frei wählbar. 
	Es genügt den Augpunkt zu verschieben. 
	Mit Hilfe der Distanz $f$, der fokalen Länge, kann ein Zoomeffekt der Kamera erzielt werden.
	
\chapter{Beleuchtungsmodell}

	Der Ausgangspunkt dieses Kapitels ist das Emission-Absorption-Modell von Nelson Max. In diesem werden die Wechselwirkungen des Lichtes auf die Absorption und die Emission beschränkt. 
	Die Streuung des Lichtes wird in dieser Arbeit vernachlässigt.
	Die Gleichung der Beleuchtungsberechnung unterteilt sich in zwei Summanden:

	\begin{equation}
		I = I_A + I_E
		\label{eq:EAModell}
 	\end{equation}
	
	Die Variable $I$ repräsentiert die von dem Sensor aufsummierte Intensität des Lichtes. 
	Der Term $I_A$ beschreibt die Abschwächung der Hintergrundbeleuchtung durch das mit Gas gefüllte Volumen und $I_E$ die Emission des Gases.
	Die Abschwächung entspricht dem Produkt des Hintergrundlichtes mit einem Wert für die Transparenz, welcher in Abhängigkeit zur Streckenlänge $D \in \mathbb{R}_+$ die das Licht durch das Volumen zurücklegen muss, steht. 
	Dieser Zusammenhang wird in der folgenden Gleichung dargestellt:
	
	\begin{equation}
		I_A = I_B \cdot T(D) 
		\label{eq:AModell}
	\end{equation}
	
	Die Transparenz wird dabei durch die Funktion $T(D)$ ermittelt:
	
	\begin{equation}
		T(s) = exp(- \int\limits_{0}^{s} \tau(t) dt)
		\label{eq:Transparenz}
	\end{equation}
	
	Der Grad der Abschwächung wird mit $\tau(t) = \sigma(\nu(t))$ bestimmt. 
	Die Funktion $\tau(t)$ wird als Transferfunktion bezeichnet, die die Dichte des Volumens auf einen Wert für die Absorption abbildet.
	Die Menge der Transferfunktionen wird in dieser Arbeit auf lineare Funktionen beschränkt. 
	$\sigma(\nu(t)) = \lambda \nu$ skaliert mit einem konstanten Faktor $\lambda \in (0,1]$ den Wert für die Dichte des Volumens.
	Diese wird mit der Funktion $\nu(t) = \kappa$ auf einen konstanten Faktor $\kappa$ abgebildet.
	Zur Vereinfachung der Formeln wird eine weitere Funktion definiert, welche die Länge aus der Differenz einer hinteren und einer 
	vorderen Position berechnet:
	
	\begin{equation}
		T'(t_n, t_f) = exp(- \int\limits_{t_n}^{t_f} (\lambda\kappa) dt)
		\label{eq:ATransparenz}
	\end{equation}
	
	In der Funktion \ref{eq:ATransparenz} steht die Variable $t_n$ für den Eintrittspunkt in das Volumen und $t_f$ für den Austrittspunkt.
	Da das Produkt von $\lambda$ und $\kappa$ ein konstanter Wert ist, kann es aus dem Integral herausgezogen werden. Somit existiert eine analytisch Lösung:
	
	\begin{equation}
		T'(t_n, t_f) = exp(-\lambda\kappa \cdot \int\limits_{t_n}^{t_f} dt) = e^{-\lambda\kappa \cdot (t_f - t_n)}
		\label{eq:ATransparenzSol}
	\end{equation}
	
	Die Emission wird mit der Variable $I_E$ bezeichnet und wird zunächst sehr allgemein definiert:	
	
	\begin{equation}
		I_E = \int\limits_{0}^{D} g(s) \cdot T'(s, D) ds
		\label{eq:EModell}
	\end{equation}
	
	Die Funktion $g(s)$ wird als Quellterm bezeichnet und stellt eine beliebige Funktion, 
	in Abhängigkeit einer Position $s$ dar. Das bei $s$ emittierte Licht wird auch durch das das Volumen abgeschwächt.
	Die Formeln für die Absorption und die Emission wird in die Gleichung \ref{eq:EAModell} eingesetzt und es ergibt sich
	das komplette Modell für die Beleuchtungsberechnung in der Form: 
	
	\begin{equation}
		I = I_B \cdot T(D) + \int\limits_{0}^{D} g(s) \cdot T'(s, D) ds
		\label{eq:CompEAModell}
	\end{equation}
	
	Die Gleichung \ref{eq:CompEAModell} ist noch sehr allgemein. Es wird beschrieben wie sich das Hintergrundlicht durch
	das Volumen abschwächt. Unter der Beschränkung auf lineare Transferfunktion existiert eine analytische Lösung. 
	Das Integral über den Quellterm wurde bisher noch nicht genauer definiert. In den nächsten beiden Absätzen
	werden zwei verschiedene Funktionen für $g(s)$ definiert.
	Beide kommen im eigentlichen Algorithmus zum Einsatz.
	
\section{Einfaches Beleuchtungs-Modell}
	
	Ein primärer Strahl, auch als Sichtstrahl bezeichnet, wird vom Sensor aus in die Szene geschossen.
	Die folgende Abbildung stellt diesen Vorgang schematisch dar. 
	
	\begin{figure}[h]
		\centering
		\def\svgwidth{10cm}
		\input{images/standartAbsorption.pdf_tex}
		\caption{Die Abbildung zeigt einen primären Sichtstrahl, der ein Partikel schneidet. Auf der rechten Seite der Abbildung symbolisiert eine kleine Sonne die Hintergrundbeleuchtung und auf der linken ein Auge den Sensor.}
		\label{fig:eaSchematisch}
	\end{figure}
	
	Der Sichtstrahl schneidet ein Partikel an den Positionen $t_n$ und $t_f$.
	Es wird eine Hintergrundbeleuchtung $I_B$ mit einer konstanten Intensität definiert.
	Diese wird entlang des Strahls durch das Gas im Volumen mit der bereits ermittelten Formel
	$I_A = I_B \cdot exp(-\lambda\kappa(t_f - t_n))$ abgeschwächt. 
	Die Transferfunktion für die Emission wird durch die Funktion $c(\nu(t)) = \lambda I_c \cdot \nu(t)$ beschrieben, wobei $I_c$ als konstante Lichtintensität gewählt wird und $\nu(t) = \kappa$ die Dichte auf den konstanten Wert
	$\kappa$ abbildet. Zunächst wird die Emissionsgleichung mit den Eintritts- und Austrittspunkten parametrisiert:
	
	\begin{equation}
		I_E = \int\limits_{t_n}^{t_f} T'(t, t_f) \cdot g(t) dt
		\label{eq:EModellEDM}
	\end{equation}
	
	Die Quellfunktion wird in diesem Modell als $g(t) = \lambda\kappa I_c$ definiert, so das die Formel für
	die Emission ausgeschrieben der folgenden Form entspricht:
	
	\begin{equation}
		I_E = \int\limits_{t_n}^{t_f} exp(-\lambda\kappa (t_f - t)) \cdot \lambda\kappa I_c dt
		\label{eq:EModellEDMA}
	\end{equation}
	
	Für die Gleichung \ref{eq:EModellEDMA} lässt sich mit Hilfe der Substitutionsregel eine analytische Lösung finden:
	
	\begin{align}
		I_E &= I_C \cdot (1 - e^{-\lambda\kappa (t_f - t_n)}) = I_C \cdot (1 - T'(t_n, t_f)) \notag\\
		I_E &= I_C \cdot \Theta(t_n, t_f)
	\end{align} 
	
	Wird diese Lösung und die Lösung \ref{eq:ATransparenzSol} in die Gleichung \ref{eq:CompEAModell} eingesetzt,
	dann ergibt sich die gesamte Gleichung zur Beleuchtung eines Partikels: 
	
	\begin{equation}
		I = I_B \cdot T'(t_n, t_f) + I_C \cdot \Theta(t_n, t_f)
		\label{eq:simpleEAModellANA}
	\end{equation}
	
\section{Erweitertes Beleuchtungs-Modell}

	Kern dieser Arbeit ist eine Erweiterung des eben vorgestellten Beleuchtungsmodells. Neben der Hintergrundbeleuchtung
	wird die Beleuchtungsberechnung für eine beliebige Anzahl von Punkt- und Richtungslichtquellen erweitert. Zur
	Herleitung der Gleichung werden die Lichtquellen auf die Hintergrundbeleuchtung und eine Punktlichtquelle beschränkt.
	Anschließend wird die gefundene Lösung wieder für beliebig viele Lichtquellen verallgemeinert.
	Die folgende Abbildung soll zunächst die Vereinfachung veranschaulichen.

	\begin{figure}[h]
		\centering
		\def\svgwidth{10cm}
		\input{images/exStandartAbsorption.pdf_tex}
		\caption{Die Notation ist die selbe wie in der Abbildung \ref{fig:eaSchematisch}. Diese Abbildung ergänzt das Modell um eine Punktlichtquelle und es werden drei sekundäre Strahlen zu dieser dargestellt.}
		\label{fig:exEaSchematisch}
	\end{figure}
	
	Die Grundlage für das erweiterte Beleuchtungs-Modell ist selbe Gleichung \ref{eq:CompEAModell}, wie für das vorherige.
	Diese wird zur besseren Übersicht an dieser Stelle mit den Notationen aus dem vorhergehenden Abschnitt wiederholt:
	
	\begin{equation}
		I = I_B \cdot T'(t_n, t_f) + \int\limits_{t_n}^{t_f} g(t) \cdot T'(t, t_f) dt
		\label{eq:CompEAModellN}
	\end{equation}
	
	Der erste Teil der Gleichung  beschreibt die Absorption und bleibt unverändert. Die Modelle unterscheiden sich nur durch die Quellfunktion $g(t)$. 
	Diese berechnet die Kugelfarbe an jedem Punkt $t \in [t_n, t_f]$ entlang des Sichtstrahls und wird anschließend durch das Volumen abgeschwächt. 
	Einfluss auf die Farbe der Kugeln hat die Intensität der Punktlichtquelle und die Emission des Gases. 
	Die Funktion $l(t)$ liefert die Schnittlänge des Sekundärstrahls mit dem Partikel. 
	Ein Sekundärstrahl ist in diesem Zusammenhang ein Strahl, welcher ausgehend von der Position $t$ in die Richtung der Punktlichtquelle geschossen wird. 
	Die Farbe an der Position $t$ bestimmt sich mit der Hilfe der Länge $l(t)$ und dem Modell aus dem vorherigen Abschnitt durch die folgende Rechenvorschrift:
	
	\begin{equation}
		g(t) = I_L \cdot T'(0, l(t)) + I_c \cdot \Theta(0, l(t))
		\label{eq:quellfunktion}
	\end{equation}
	
	Die Variable $I_L$ entspricht der Intensität der Punktlichtquelle.
	Punkt und Richtungslichtquellen unterscheiden sich in der Berechnung des Richtungsvektors für die Sekundärstrahlen.
	Der Richtungsvektor der Sekundärstrahlen für Punktlichtquellen bestimmt sich aus der normierten Differenz zwischen der Position der Lichtquelle und der Position auf dem Sichtstrahl in Abhängigkeit von $t$. 
	Richtungslichtquellen werden bereits mit einem normierten Richtungsvektor parametrisiert. Die Invertierung von diesem entspricht dem Richtungsvektor des Sekundärstrahls. Wird die Gleichung \ref{eq:quellfunktion} in die Gleichung \ref{eq:CompEAModellN} eingesetzt, ergibt sich daraus ein Integral für das keine analytische Lösung existiert:
	
	\begin{equation}
		I = I_B \cdot T'(t_n, t_f) + \int\limits_{t_n}^{t_f} ( I_L \cdot T'(0, l(t)) + I_c \cdot \Theta(0, l(t)) ) \cdot T'(t, t_f) dt
		\label{eq:CompEAModell1}
	\end{equation}
	
	Die Lösung der Gleichung wird mit Hilfe der Riemann Summe numerisch approximiert. Bevor die Lösung präsentiert
	und die Quellfunktion für mehrere Lichtquellen angepasst wird, soll die numerische Approximation anhand einer
	fiktiven Funktion $f$ beschrieben werden.
	Dazu wird das Integral der Funktion $f$ zu einer Summe von skalierten Funktionswerten überführt:
	
	\begin{align}
		\int\limits_{0}^{D} f(x) dx \approx \sum\limits_{i = 1}^{n} f(x_i) \Delta x
	\end{align}
	
	Das Intervall von $0$ bis $D$ wird in $n$ kleinere Teile zerlegt, wobei jedes dieser Teilintervalle die selbe Länge $\Delta x = \frac{D}{n}$ besitzt. 
	Aus jedem Intervall $i$ wird ein beliebiges Element $x_i$ aus diesem Intervall gewählt und der Funktionswert $f(x_i)$ mit $\Delta x$ skaliert und auf das Ergebnis aufsummiert. 

	Demnach lässt sich die Formel \ref{eq:CompEAModellN} zu der folgenden Gleichung vereinfachen:
	
	\begin{equation}
		I = I_B \cdot T'(t_n, t_f) + \sum\limits_{t = t_n}^{t_f} g(t) \cdot T'(t, t_f) \Delta t
		\label{eq:CompEAModellApprox1}
	\end{equation}
	
	Nun wird eine Anzahl von $n$ Stützpunkten zur Regulierung der Güte der Approximation definiert. 
	Generell gilt, dass	je größer der Wert von $n$ ist um so besser wird das Integral approximiert. 
	Der Wert für die Skalierung bestimmt sich als $\Delta t = \frac{t_f - t_n}{n}$.
	
	Im Folgenden entspricht die Menge $L = \{L_1, L_2, \cdots L_m\}$ der Menge der Lichtquellen. 
	Jede von ihnen wird über den dazugehörigen Index identifiziert. 
	Die Intensitäten der Lichtquellen werden mit dem Formelzeichen $I_{L_i}$ symbolisiert. 
	Dabei entspricht $i$ dem Index der dazugehörigen Lichtquelle.
	Damit kann der Quellterm erweitert werden, so dass die Formel für die Beleuchtungsberechnung lautet:
	
	\begin{equation}
		I = I_B \cdot T'(t_n, t_f) + \sum\limits_{t = t_n}^{t_f} ( \frac{1}{m} \cdot \sum\limits_{i=1}^{m}I_{L_i} \cdot T'(0, l(t)) + I_c \cdot \Theta(0, l(t))) \cdot T'(t, t_f) \Delta t
		\label{eq:CompEAModellApprox2}
	\end{equation}

\chapter{Implementierung}

	Die beschriebenen Beleuchtungsmodelle wurden für den Schnitt eines Strahls mit einem Partikel aufgestellt. 
	In diesem Kapitel werden die Grundlagen und die beiden Beleuchtungsmodelle zusammengeführt.
	Es wird erläutert, wie die Bilder im Detail erzeugt werden und welche Vereinfachungen die Berechnung beschleunigen.
	
	Die zu zeichnende Szene besteht aus einer Menge von Partikeln, dargestellt als Kugeln.
	Jede Kugel wird durch ihre Position und einen Radius parametrisiert. 
	Zusätzlich wird für jede Kugel ein Material definiert, welches sich aus einer Farbe und ein Wert für die maximale Durchlässigkeit zusammensetzt. 
	Die maximale Durchlässigkeit wird dabei durch das Symbol $\Theta_{max} \in [0, 1]$ repräsentiert.
	Aus $\Theta_{max}$ wird für jede Kugel der Wert für $\kappa$ nach der folgenden Vorschrift berechnen:
	
	\begin{equation}
		\kappa = -\frac{1}{\lambda \cdot 2r} \ln(1-\Theta_{max})
		\label{eq:maxOpacity}
	\end{equation}
	
	Der Parameter $\lambda$ wird für alle Kugeln global gewählt und liegt im Intervall $(0,1]$. 
	Neben der Hintergrundbeleuchtung gibt es die Möglichkeit beliebig viele Punkt- und Richtungslichtquellen zu definieren.
	Eine Punktlichtquelle besitzt neben ihrer Position eine Farbe. 
	Eine Richtungslichtquelle besitzt keine Position, sondern einen normierten Richtungsvektor und eine Farbe als Parameter.
	
	Die Auflösung eines Bildes entspricht $W \times H$. 
	Für jeden Bildpunkt $(x, y) \in W \times H$ wird ein primärer Strahl erzeugt, dessen Ursprung der Kameraposition entspricht.
	Der Richtungsvektor lässt sich anhand der Vorschrift \ref{eq:direction} bestimmen.
	Das Resultat dieses Schrittes ist eine Liste der primären Strahlen.
	
	Ein großer Vorteil der Raycast-Technik besteht in der Möglichkeit die Berechnungen zu parallelisieren,
	da die Beleuchtungsberechnung für jeden Strahl unabhängig voneinander durchgeführt werden kann. 
	In der eigentlichen Implementierung wird die Parallelisierung mit Hilfe der OpenMP-Bibliothek umgesetzt.
	
	Für die Sichtstrahlen wird die Beleuchtung durch das erweiterte Beleuchtungsmodell berechnet. 
	Dazu muss für jeden Strahl ein Schnittest mit allen Kugeln durchgeführt werden.
	Um die Anzahl der Kollisionstests auf ein Minimum zu reduzieren, wird eine Beschleunigungsdatenstruktur in Form
	eines KD-Baums eingesetzt. Dieser wird direkt nach dem Laden der Szene konstruiert.
	Sobald alle Kugeln bestimmt worden sind, die von einem Sichtstrahl geschnitten worden, müssen diese entlang
	des Strahls sortiert werden.
	Anschließend wird für jede von diesen Kugeln, die erweiterte Beleuchtungsberechnung durchgeführt.
	Dabei wird mit der Kugel begonnen, die am weitesten vom Ursprung des Strahls entfernt liegt. 
	Das Resultat der Beleuchtungsberechnung, für diese Kugel, wird als neue Hintergrundbeleuchtung für die nächste eingesetzt. 
	Dieser Vorgang wird wiederholt, bis alle Kugeln entlang des Strahls abgearbeitet wurden.
	Am Sensor wird die Intensität der letzten Beleuchtungsberechnung gespeichert und anschließend normiert.
	
	Da sich für die erweiterte Beleuchtungsberechnung keine analytische Lösung finden ließ,
	muss für jeden Stützpunkt ein neuer Strahl in Richtung jeder Lichtquelle geschossen werden.
	Diese werden als Sekundärstrahlen bezeichnet. 
	Für jeden Sekundärstrahl wird das Licht, der entsprechenden Lichtquelle, als Hintergrundbeleuchtung betrachtet.
	Die Intensität dieses Lichtes wird entlang des Strahls wiederum durch Partikel, 
	aber dieses mal nach dem vereinfachten Beleuchtungsmodell abgeschwächt.
	Um dies zu bewerkstelligen ist es nötig für jeden Sekundärstrahl abermals den Schnitt mit allen Kugeln zu bestimmen 
	und es wieder ist eine Sortierung der Ergebnismenge entlang des Strahls erforderlich. 
	
	Die Geschwindigkeit des Algorithmus hängt entscheidend von der Auflösung des Bildes und der damit verbunden Anzahl von Primärstrahlen ab.
	Entscheidend für Berechnungsdauer ist die Wahl der Stützpunktanzahl, da für jeden ein Sekundärstrahl ein Schnitt mit allen Kugeln
	geprüft werden muss.
	
	An dieser Stelle wird ein weiterer Parameter $N$ eingeführt. 
	Der Wert von $N$ bestimmt, für wie viele Kugeln entlang des Primärstrahls, aber dieses mal ausgehend von dem Ursprung des Strahls, 
	die Beleuchtung mit der erweiterten Formel bestimmt werden soll.
	Mit Hilfe dieser Approximation lässt sich die Anzahl der zu erzeugenden Sekundärstrahlen drastisch reduzieren und dadurch die Dauer
	der Berechnung verringern.
	Im nächsten Kapitel wird der Einfluss, den diese Vereinfachung auf das Ergebnisbild
	hat evaluiert.

\chapter{Evaluation und Diskussion}

	Zu beginn der Arbeit wurden die Mathematischen Grundlagen der Raycast-Technik hergeleitet und 
	im Anschluss zwei Lösungen für die Volumengleichung besprochen. Die Idee bestand darin die Gleichung,
	für die Beleuchtung, weitestgehend analytisch zu lösen. 
	Das einfache Modell kann komplett analytisch gelöst werden.
	Für das erweiterte ist eine numerische Approximation von Nöten, dazu wurde das Integral in Riemann Summen zerlegt. 
	Es wurde eine Vereinfachung zur Beschleunigung der Bildsynthese vorgestellt, diese wird im Folgenden mit
	Ergebnissen verglichen, die ohne Vereinfachung erzeugt worden sind.  
	
	Zur Auswertung wurden zwei Datensätze in verschiedenen Szenen gezeichnet und die Rechenzeiten erhoben.
	Die zur Evaluation erhoben Daten wurden mit einem Core i7-4700HQ CPU (2.40GHz x 8) berechnet und die
	Bilder mit einer Auflösung von $512 \times 512$ erzeugt. Bei dem ersten Datensatz handelt es sich
	um das 1UUN-Protein, das mit 2758 Partikeln dargestellt wird. Der zweite Datensatz wurde mit
	einem Zufallsgenerator generiert und besteht aus 73642 Partikeln.
	Geschwindigkeit und Güte des Algorithmus werden in Abhängigkeit der Stützpunktanzahl und $N$ evaluiert.
	Die Abkürzung GT steht im weiteren Textverlauf für Ground-Truth und bezeichnet eine Bildsynthese
	ohne das Beschleunigungsverfahren.
	
	\begin{figure}[h]
		\subfigure[5 Stützpunkte und $N$ = 1]{\includegraphics[width=0.49\textwidth]{images/512x512_samples_5_approx_1.png}}
	    \subfigure[100 Stützpunkte und $N$ = GT]{\includegraphics[width=0.49\textwidth]{images/512x512_samples_100_approx_100000.png}}
		\caption{Abbildungen des 1UUN-Proteins, die mit verschiedenen Parametern erzeugt wurden.}
		\label{fig:1UUN}
	\end{figure}
	
	Es wird das 1UUN-Protein betrachtet, die Abbildung \ref{fig:1UUN} zweigt zwei Abbildungen des Proteins,
	gezeichnet mit zwei unterschiedlichen Parameter Einstellungen. 
	Das Bild auf der linken Seite wurde mit einer sehr niedrigen Anzahl von Stützpunkten generiert 
	und verwendet nur für die erste, vom Sichtstrahl geschnittene Kugel, das erweiterte Beleuchtungsmodell.
	Das zweite Bild auf der rechten Seite wurde mit 100 Stützpunkten je geschnittener Kugel generiert und verwendet für alle Kugeln, geschnitten vom Sichtstrahl, das erweiterte Beleuchtungsmodell.
	Die zu diesen Datensatz erhobenen Rechenzeiten sind im Folgenden Diagramm abgebildet:

	\begin{figure}[h]
		\centering
		\includegraphics[width=10cm]{images/pdb1UUN.pdf}
		\caption{Das Diagramm stellt die Renderzeiten in Abhängigkeit der gewählten Parmeter 1UUN-Proteins dar.}
		\label{fig:pdb.1UUNChart}
	\end{figure}
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=10cm]{images/cool_random.pdf}
		\caption{Das Diagramm stellt die Renderzeiten in Abhängigkeit der Stützpunkanzahl und $N$, des Datensatzes pdb.1UUN dar.}
		\label{fig:cool_randomChart}
	\end{figure}
	
	

	
\section{Fazit}

%\chapter{title}
%
%\chapter{ein kapitel}
%\section{eine Grafik}
%\begin{figure}[htbp]
%	\centering
%		\includegraphics{test.png}
%	\caption{beschriftung}
%	\label{fig:diplominf}
%\end{figure}

\end{document}